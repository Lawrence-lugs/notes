[
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#general-frequency-response",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#general-frequency-response",
    "title": "Transistor Capacitances",
    "section": "General Frequency Response",
    "text": "General Frequency Response\nMost amplifiers have a frequency response that looks like the following:"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#everything-is-a-capacitor",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#everything-is-a-capacitor",
    "title": "Transistor Capacitances",
    "section": "Everything is a capacitor",
    "text": "Everything is a capacitor\n\nIf something holds charge, it is a capacitor.\n\\[C = \\frac{dQ}{dV} \\]\n\n\nHence, everywhere in semiconductor devices where there are holes and electrons moving in and out there is capacitance."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#bjt-parasitics-1",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#bjt-parasitics-1",
    "title": "Transistor Capacitances",
    "section": "BJT Parasitics",
    "text": "BJT Parasitics\n\n\nThere are two main parasitic capacitances in a BJT:\n\n\nBase-Emitter Capacitance \\(C_{\\pi} = C_b + C_{je}\\) (Base Charging Capacitance + Base-Emitter Junction Capacitance)\nBase-Collector Capacitance \\(C_{\\mu}\\)"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-base-charging-capacitance-c_b",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-base-charging-capacitance-c_b",
    "title": "Transistor Capacitances",
    "section": "The base charging capacitance \\(C_b\\)",
    "text": "The base charging capacitance \\(C_b\\)\n\nBefore we start using the BJT, there is effectively “no charge” in the base region.\n\n\nHowever, as we turn the BJT on, the emitter starts to inject carriers (charge) into the base."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-base-charging-capacitance-c_b-1",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-base-charging-capacitance-c_b-1",
    "title": "Transistor Capacitances",
    "section": "The base charging capacitance \\(C_b\\)",
    "text": "The base charging capacitance \\(C_b\\)\n\nThe amount of carrier charge entering/exiting the base per second is \\(I_{C}\\)\n\n\nIf the amount of time a carrier spends in the base is \\(\\tau_F\\), then the total charge stored in the base is: \\[ Q_B =  I_C \\tau_F \\]\n\n\nSo then a change in \\(V_{BE}\\) can cause a change in \\(Q_B\\) through \\(I_C\\):\n\n\n\n\\[ C_b = \\frac{dQ_B}{dV_{BE}} \\]\n\n\n\\[ C_b = \\frac{dQ_B}{dV_{BE}} = \\frac{d(I_C \\tau_F)}{dV_{BE}} \\]\n\n\n\\[ C_b = \\frac{dQ_B}{dV_{BE}} = \\frac{d(I_C \\tau_F)}{dV_{BE}} = \\tau_F \\frac{dI_C}{dV_{BE}} \\]\n\n\n\nHence,\n\\[ C_b = \\tau_F g_m \\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-be-junction-capacitance-c_je",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-be-junction-capacitance-c_je",
    "title": "Transistor Capacitances",
    "section": "The BE Junction Capacitance \\(C_{je}\\)",
    "text": "The BE Junction Capacitance \\(C_{je}\\)\n\n\nA depletion region is a region with charges.\n\n\n\n\n\n\nAs we discussed before, the width of the depletion region changes with applied voltage. It is known that \\(W_{dep} \\propto \\sqrt{V_{bi} - V_{BE}}\\)."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-be-junction-capacitance-c_je-1",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-be-junction-capacitance-c_je-1",
    "title": "Transistor Capacitances",
    "section": "The BE Junction Capacitance \\(C_{je}\\)",
    "text": "The BE Junction Capacitance \\(C_{je}\\)\n\nHence, since \\(Q_{dep} \\propto W_{dep}\\), we have that some constant of proportionality \\(\\alpha\\) exists such that:\n\n\n\\[ Q_{dep} = \\alpha \\sqrt{V_{bi} - V_{BE}} \\]\n\n\n\\[ C_{je} = \\left|\\frac{dQ_{dep}}{dV_{BE}}\\right| = \\left|\\alpha \\frac{d(\\sqrt{V_{bi} - V_{BE}})}{dV_{BE}}\\right| = \\left|-\\frac{\\alpha}{2\\sqrt{V_{bi} - V_{BE}}}\\right|  = \\frac{\\alpha}{2\\sqrt{V_{bi}}\\sqrt{1 - V_{BE}/V_{bi}}}\\]\n\n\nWe can treat \\(\\frac{\\alpha}{2\\sqrt{V_{bi}}}\\) as a constant \\(C_{je0}\\):\n\\[ C_{je} = \\frac{C_{je0}}{\\sqrt{1 - V_{BE}/V_{bi}}} = \\frac{C_{je0}}{\\sqrt{1 - V_{BE}/V_{j,BE}}} \\]\n\n\nand we see that \\(C_{je0}\\) is base-emitter junction capacitance measured at \\(V_{BE} = 0\\) V. We also typically refer to \\(V_{bi}\\) for the BE junction as \\(V_{j,BE}\\)."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-be-junction-capacitance-c_je-2",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-be-junction-capacitance-c_je-2",
    "title": "Transistor Capacitances",
    "section": "The BE Junction Capacitance \\(C_{je}\\)",
    "text": "The BE Junction Capacitance \\(C_{je}\\)\n\\[\\frac{C_{je0}}{\\sqrt{1 - V_{BE}/V_{j,BE}}}\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-bc-junction-capacitance-c_mu",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-bc-junction-capacitance-c_mu",
    "title": "Transistor Capacitances",
    "section": "The BC Junction Capacitance \\(C_{\\mu}\\)",
    "text": "The BC Junction Capacitance \\(C_{\\mu}\\)\nLikewise, the BC junction’s depletion region also has a capacitance. We call this \\(C_{\\mu}\\).\nSince it’s usually reverse biased, we replace \\(V_{BE}\\) with \\(V_{CB}\\) and negate the sign in the denominator:\n\\[ C_{\\mu} = \\frac{C_{\\mu0}}{\\sqrt{1 + V_{CB}/V_{j,CB}}} \\]\n\n\n\n\n\n\n\nNote\n\n\nWe call it \\(C_\\mu\\) and not \\(C_{jc}\\) because of its position in the small-signal equivalent. \\(\\mu\\) is traditionally used to represent voltage feedback from output to input in the hybrid-pi model."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#bjt-parasitics-summary",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#bjt-parasitics-summary",
    "title": "Transistor Capacitances",
    "section": "BJT Parasitics Summary",
    "text": "BJT Parasitics Summary\n\n\n\n\n\\(C\\mu\\) - Junction capacitance between B and C\n\\(C_\\pi = C_b + C_{je}\\)\n\n\\(C_b\\) - Capacitance from carriers crossing the base\n\\(C_{je}\\) - Junction capacitance between B and E\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\\(C_\\mu\\) and is typically around 5fF-10fF.\n\\(C_{je}\\) is a little higher, about \\(&gt;10fF\\).\n\\(C_b\\) is typically around hundreds of fF."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#bjt-small-signal-model-with-capacitances",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#bjt-small-signal-model-with-capacitances",
    "title": "Transistor Capacitances",
    "section": "BJT Small Signal Model with Capacitances",
    "text": "BJT Small Signal Model with Capacitances\n\n\n\n\\[C\\pi=C_b+C_{je}\\] \\[C_b=\\tau_Fg_m\\]\n\n\\[\\frac{C_{je0}}{\\sqrt{1 - V_{BE}/V_{j,BE}}}\\]\n\n\\[ C_{\\mu} = \\frac{C_{\\mu0}}{\\sqrt{1 + V_{CB}/V_{j,CB}}} \\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#mos-parasitics-1",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#mos-parasitics-1",
    "title": "Transistor Capacitances",
    "section": "MOS Parasitics",
    "text": "MOS Parasitics\n\n\nThe MOS has three sources of parasitics:\n\nDrain/Source to Bulk Junction Capacitance \\(C_{db}\\),\\(C_{sb}\\)  - Same as earlier.\nGate-bulk capacitance \\(C_{gb}\\) - The MOS Capacitor. Remember this?\nGate-overlap capacitance \\(C_{gs}\\)  - MOS Capacitor but we accidentally do it to S/D."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#mos-parasitics-2",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#mos-parasitics-2",
    "title": "Transistor Capacitances",
    "section": "MOS Parasitics",
    "text": "MOS Parasitics"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#overlap-capacitances-c_gs-and-c_gd",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#overlap-capacitances-c_gs-and-c_gd",
    "title": "Transistor Capacitances",
    "section": "Overlap Capacitances \\(C_{GS}\\) and \\(C_{GD}\\)",
    "text": "Overlap Capacitances \\(C_{GS}\\) and \\(C_{GD}\\)\n\n\n\n\n\\(C_{GS}\\) and \\(C_{GD}\\) are overlap capacitances caused by an effective MOS capacitor on the overlap of the G region to the S and D regions.\n\n\n\\[C_{GS} = C_{GD} = \\frac{\\epsilon_{ox} x_d W}{t_{ox}}\\]\n\n\nBut if you may remember…\n\n\n\\[C_{ox}=\\frac{\\epsilon_{ox}}{t_{ox}}\\]\n\n\n\\[C_{GS} = C_{GD} = C_{ox}x_dW\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-gate-bulk-capacitance-c_gb",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-gate-bulk-capacitance-c_gb",
    "title": "Transistor Capacitances",
    "section": "The Gate-bulk Capacitance \\(C_{gb}\\)",
    "text": "The Gate-bulk Capacitance \\(C_{gb}\\)\n\n\n\nAs we discussed before, the main physical device behind the MOSFET is a MOS capacitor.\n\n\n\\[ C_{gb} = \\frac{\\epsilon_{ox}}{t_{ox}}WL \\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-drain-bulk-and-source-bulk-capacitances-c_dbc_sb",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-drain-bulk-and-source-bulk-capacitances-c_dbc_sb",
    "title": "Transistor Capacitances",
    "section": "The Drain-bulk and Source-bulk Capacitances \\(C_{db}\\),\\(C_{sb}\\)",
    "text": "The Drain-bulk and Source-bulk Capacitances \\(C_{db}\\),\\(C_{sb}\\)\n\\(C_{db}\\) and \\(C_{sb}\\) come from junction capacitances of the PN junction formed by the opposite-type semiconductors of the drain/source and the bulk."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#mosfet-capacitances",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#mosfet-capacitances",
    "title": "Transistor Capacitances",
    "section": "MOSFET Capacitances",
    "text": "MOSFET Capacitances\n\n\nMOS Parallel Plate Capacitances\n\n\\(C_{gs}\\)\n\\(C_{gd}\\)\n\\(C_{gb}\\)\n\nS/D-Bulk Junction Capacitances\n\n\\(C_{sb}\\)\n\\(C_{db}\\)"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#effective-mos-small-signal-with-capacitances",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#effective-mos-small-signal-with-capacitances",
    "title": "Transistor Capacitances",
    "section": "Effective MOS Small-signal with Capacitances",
    "text": "Effective MOS Small-signal with Capacitances\n\n\nThe bulk is typically connected to the source meaning:\n\n\n\n\\(C_{gs}\\) typically also accounts for \\(C_{gb}\\).\n\n\n\n\n\\(C_{sb}\\) is shorted.\n\n\n\nSo, only \\(C_{gs}\\), \\(C_{gd}\\) and \\(C_{db}\\) survive."
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-fundamental-gains",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-fundamental-gains",
    "title": "Transistor Capacitances",
    "section": "The fundamental gains",
    "text": "The fundamental gains\nLet’s see what happens to the fundamental gains due to the capacitances:\n\n\n\nVoltage gain:\n\n\n\\[ \\frac{v_o}{r_o || 1/sC\\mu} + v_i (g_m + sC\\mu) = 0 \\]\n\n\n\\[ \\frac{v_o}{v_i} = - (g_m + sC\\mu)(\\frac{r_o}{1+sC\\mu r_o}) \\]\n\n\n\\[ \\frac{v_o}{v_i} = - g_mr_o \\frac{(1 + sC\\mu/g_m)}{(1+sC\\mu r_o)} \\]\n\n\n\n\n\n\nCurrent gain:\n\n\n\\[ i_o = v_i (g_m - sC\\mu) \\]\n\n\n\\[ i_i = v_i (sC\\pi + sC\\mu + 1/r_{\\pi}) \\]\n\n\n\\[ \\frac{i_o}{i_i} = \\frac{g_m-sC\\mu}{1/r_{\\pi} + sC_\\pi+sC_\\mu}\\]\n\n\n\\[ \\frac{i_o}{i_i} = g_mr_{\\pi} \\frac{1-sC\\mu/g_m}{1+sr_{\\pi}(C_\\pi+C_\\mu)}\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t",
    "title": "Transistor Capacitances",
    "section": "The Transition Frequency \\(f_T\\)",
    "text": "The Transition Frequency \\(f_T\\)\n\n\n\nYou’ll notice that the current gain reaches 1 (0 dB), but the voltage gain doesn’t.\n\n\nHence, people measure the “speed of the transistor” or the transition frequency as \\[ \\left| \\frac{i_o}{i_i} \\right|_{f_T} = 1\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-1",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-1",
    "title": "Transistor Capacitances",
    "section": "The Transition Frequency \\(f_T\\)",
    "text": "The Transition Frequency \\(f_T\\)\n\n\n\\[ \\frac{i_o}{i_i} = g_mr_\\pi \\frac{1-sC\\mu/g_m}{s(C_\\pi+C_\\mu)}\\]\n\n\n\\[ \\frac{i_o}{i_i} = \\frac{g_m-sC\\mu}{1/r_\\pi + s(C_\\pi+C_\\mu)}\\]\n\n\n\\[ \\left| \\frac{i_o}{i_i} \\right| = \\frac{\\sqrt{g_m^2 + \\omega_T^2C_\\mu^2}}{\\sqrt{(1/r_\\pi)^2 + \\omega_T^2(C_\\pi+C_\\mu)^2}} = 1\\]\n\n\n\\[ \\frac{g_m^2 + \\omega_T^2C_\\mu^2}{(1/r_\\pi)^2 + \\omega_T^2(C_\\pi+C_\\mu)^2} = 1\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-2",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-2",
    "title": "Transistor Capacitances",
    "section": "The Transition Frequency \\(f_T\\)",
    "text": "The Transition Frequency \\(f_T\\)\n\n\n\\[ \\frac{g_m^2 + \\omega_T^2C_\\mu^2}{(1/r_\\pi)^2 + \\omega_T^2(C_\\pi+C_\\mu)^2} = 1\\]\n\n\n\\[ \\omega_T^2 = \\frac{g_m^2 - (1/r_\\pi)^2}{(C_\\pi+C_\\mu)^2 - C_\\mu^2}\\]\n\n\nSince \\(g_m^2 \\gg 1/r_\\pi^2\\) and \\((C_\\pi+C_\\mu)^2 \\gg C_\\mu^2\\)\n\n\n\n\n\n\nDefinition\n\n\n\\[\\omega_T \\approx \\frac{g_m}{C_\\pi + C_\\mu}, f_T \\approx \\frac{g_m}{2\\pi(C_\\pi + C_\\mu)}\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-3",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-3",
    "title": "Transistor Capacitances",
    "section": "The Transition Frequency \\(f_T\\)",
    "text": "The Transition Frequency \\(f_T\\)\nFor a MOSFET, everything is the same.\nSet \\(r_\\pi \\rightarrow \\infty\\)\n\n\n\nDefinition\n\n\n\\[\\omega_T \\approx \\frac{g_m}{C_{gd} + C_{gs}}, f_T \\approx \\frac{g_m}{2\\pi(C_{gd} + C_{gs})}\\]"
  },
  {
    "objectID": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-4",
    "href": "slides/transistorCapacitances/transistorCapacitances.html#the-transition-frequency-f_t-4",
    "title": "Transistor Capacitances",
    "section": "The Transition Frequency \\(f_T\\)",
    "text": "The Transition Frequency \\(f_T\\)\n\n\n\n\nTransistor Capacitances"
  },
  {
    "objectID": "notes/wavefunctionsBrasKets.html",
    "href": "notes/wavefunctionsBrasKets.html",
    "title": "Wavefunctions, Bras, Kets",
    "section": "",
    "text": "References\n\n\n\n\nAllan Adams MIT 8.04\nLancaster, Tom, and Stephen J. Blundell. Quantum field theory for the gifted amateur. OUP Oxford, 2014."
  },
  {
    "objectID": "notes/wavefunctionsBrasKets.html#how-is-this-related-to-the-fourier-transform-though",
    "href": "notes/wavefunctionsBrasKets.html#how-is-this-related-to-the-fourier-transform-though",
    "title": "Wavefunctions, Bras, Kets",
    "section": "How is this related to the fourier transform though?",
    "text": "How is this related to the fourier transform though?\nFirst, It’s a fundamental observation that waves have energy \\[E=\\hbar\\omega=h\\nu\\] \\[p=\\hbar k=h/\\lambda\\] Then, DeBroglie postulated that everything obeys this law.\nSecondly, we know a correspondence between the wavefunction \\(\\psi(x)\\) and the momentum. Quite easily, if \\[\\psi(x)=e^{-ikx}\\] Then we can identify \\(k\\) as the wavelength as postulated earlier.\n\nThen, since all valid wavefunctions can be formed from superpositions, then we can sum an infinite number of plane waves whose \\(k\\) is definite to form an arbitrary wavefunction with a “hard-to-determine” \\(k\\), as in Fourier’s theorem. \\[\\psi(x) = \\frac{1}{2\\pi}\\int^{\\infty}_{-\\infty}\\psi(k)e^{ikx}dx\\] ::: {.callout-important title=“Wait…”} Where does \\(1/2\\pi\\) come from in this derivation? Probably some sort of normalization. :::"
  },
  {
    "objectID": "notes/The Generalized Stokes' Theorem.html",
    "href": "notes/The Generalized Stokes' Theorem.html",
    "title": "The Generalized Stokes’ Theorem",
    "section": "",
    "text": "The generalized Strokes’ Theorem relates the 3D divergence theorem, the 2D Stokes’ theorem and the second fundamental theorem of calculus to all dimensions in a single statement: \\[\\int_{d\\Omega}\\omega=\\int_\\Omega d\\omega\\] ::: {.callout-quote title=“Quote”} The integral of a differential form \\(\\omega\\) over the boundary \\(d\\Omega\\) of some orientable manifold \\(\\Omega\\) is equal to the integral of its exterior derivative \\(d\\omega\\) over the whole of \\(\\Omega\\). :::\n\nAgainst the second fundamental theorem of Calculus\nThe integral between some domain bounds \\([A,B]\\) of \\(f\\) is \\(F(A)\\) and \\(F(B)\\) such that \\(F\\) is the antiderivative of \\(f\\): \\[dF=fdx\\] So then here, \\(\\Omega = [A,B]\\) \\[\\int_\\Omega dF = \\int_{\\Omega}fdx\\] So then what does \\(d\\Omega\\) mean in this case? \\(d\\Omega = \\{A,B\\}\\) is the set containing A and B (just points). So then the integral involving just points is… just an evaluation on those points? \\[\\int_\\Omega fdx=\\int_{\\partial[A,B]}F=F(B)-F(A)\\] ::: {.callout-warning title=“Extra step?”} The wikipedia derivation has an extra step \\[\\int_{\\{a\\}^-\\cup\\{b\\}^+}F\\] So this is like an infinite sum of just \\(F\\) evaluated on \\(a\\) and \\(b\\)? It doesn’t need a \\(dx\\) or anything because it won’t blow up to infinity, being evaluated on just points. :::"
  },
  {
    "objectID": "notes/Running TinyGPU by Adam Maj.html",
    "href": "notes/Running TinyGPU by Adam Maj.html",
    "title": "Running TinyGPU by Adam Maj",
    "section": "",
    "text": "Let’s try to run the basic GPU example by Adam Maj! Assuming WSL…\n\n\n\n\n\n\nWarning\n\n\n\nIf not running from WSL, you may need to change the docker compose script. So far though, nothing really needs an XServer so you probably don’t\n\n\n\nEnvironment Setup\nFirst, start from the custom (with sudo) iic-osic-tools image. This custom image:\n\nAdds sudo capability.\nUses the NVIM deployment script\n\nFROM hpretl/iic-osic-tools:latest\n\n\n# Install sudo\nUSER root\n\nRUN useradd -s /bin/bash designer\nRUN usermod -aG sudo designer\n\nRUN echo \"designer ALL=(ALL) NOPASSWD:ALL\" &gt; /etc/sudoers.d/designer \\\n    && chmod 0440 /etc/sudoers.d/designer\n\nWORKDIR /workspaces\n\nUSER designer\n\n\n# Deploy NVIM\n\n# ==============================================================================\n\n# [vim_deploy] INJECTION START\n\n# ==============================================================================\n\n# 1. Install prerequisites\nRUN sudo apt-get update && sudo apt-get install -y git curl wget\n\n\n# 2. Clone and Run the Installer\n\n# NOTE: We clone to a permanent path (/root/.vim_deploy) and DO NOT delete it.\n\n# The install.sh creates symlinks to this folder. If we deleted it, the config would break.\nRUN git clone https://github.com/Lawrence-lugs/vim_deploy ~/.vim_deploy && \\\n    cd ~/.vim_deploy && \\\n    bash install.sh\n\n\n# 3. Update PATH and ALIASES for the image\nENV PATH=\"~/miniforge3/bin:$PATH\"\n\n\n# (Optional) Ensure the alias works for the default user in interactive mode\nRUN echo \"alias vim=nvim\" &gt;&gt; ~/.bash_aliases\n\n# ==============================================================================\n\n# [vim_deploy] INJECTION END\n\n# ==============================================================================\ndocker-compose.yml\nservices:\n  osic_tools:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    container_name: osic_tools_container\n    working_dir: /workspaces\n    environment:\n      - DISPLAY=:0\n      - WAYLAND_DISPLAY=wayland-0\n      - XDG_RUNTIME_DIR=/mnt/wslg/runtime-dir\n      - PULSE_SERVER=/mnt/wslg/PulseServer\n      - SHELL=/bin/bash\n    volumes:\n      - ../:/workspaces\n      # WSLg Graphics & Audio Sockets\n      - /run/desktop/mnt/host/wslg/.X11-unix:/tmp/.X11-unix\n      - /run/desktop/mnt/host/wslg:/mnt/wslg\n      # Hardware Acceleration (GPU Drivers)\n      - /usr/lib/wsl:/usr/lib/wsl\n\nThen do docker compose up to open a terminal for use.\n\nSomehow, after the deploy script is done, we still need to redo mamba init due to some inner workings in iic-osic-tools. We need to activate the deployed mamba init: do…\nThen, we need to install cocotb using miniforge mamba (conda but better). However, the latest versions of cocotb no longer have the build command cocotb-config --prefix used in the TinyGPU Makefile so we need a specific version\ntest_%:\n    make compile\n    iverilog -o build/sim.vvp -s gpu -g2012 build/gpu.v\n    MODULE=test.test_$* vvp -M $$(cocotb-config --prefix)/cocotb/libs -m libcocotbvpi_icarus build/sim.vvp\n\nmamba create -n \"cocotb\" cocotb==1.7.2 pytest mamba activate cocotb\n\nWe also need sv2v (systemVerilog to Verilog converter)\nwget https://github.com/zachjs/sv2v/releases/download/v0.0.13/sv2v-Linux.zip\nunzip sv2v-Linux.zip\n\nProblem 1: Mamba is trying to use the base env re instead of the cocotb env re\n  File \"/headless/miniforge3/lib/python3.13/re/_compiler.py\", line 18, in &lt;module&gt;\n    assert _sre.MAGIC == MAGIC, \"SRE module mismatch\"\n           ^^^^^^^^^^^^^^^^^^^\nAssertionError: SRE module mismatch\nThis is because $PYTHONPATH is explicitly set in the iic-osic-tools container. We need to unset it.\n\nunset PYTHONPATH\n\nUnfortunately, you have to do this everytime after activating.\n\n\n\nRunning the tests\n\nmake test_matmul\n\nIt takes a LONG time but finally,\n(cocotb) /workspaces/tiny-gpu &gt; make test_matmul\nmake compile\nmake[1]: Entering directory '/workspaces/tiny-gpu'\nmake compile_alu\nmake[2]: Entering directory '/workspaces/tiny-gpu'\nsv2v -w build/alu.v src/alu.sv\nmake[2]: Leaving directory '/workspaces/tiny-gpu'\nsv2v -I src/* -w build/gpu.v\necho \"\" &gt;&gt; build/gpu.v\ncat build/alu.v &gt;&gt; build/gpu.v\necho '`timescale 1ns/1ns' &gt; build/temp.v\ncat build/gpu.v &gt;&gt; build/temp.v\nmv build/temp.v build/gpu.v\nmake[1]: Leaving directory '/workspaces/tiny-gpu'\niverilog -o build/sim.vvp -s gpu -g2012 build/gpu.v\nMODULE=test.test_matmul vvp -M $(cocotb-config --prefix)/cocotb/libs -m libcocotbvpi_icarus build/sim.vvp\n     -.--ns INFO     gpi                                ..mbed/gpi_embed.cpp:76   in set_program_name_in_venv        Did not detect Python virtual environment. Usin\ng system-wide Python interpreter\n     -.--ns INFO     gpi                                ../gpi/GpiCommon.cpp:101  in gpi_print_registered_impl       VPI registered\n     0.00ns INFO     cocotb                             Running on Icarus Verilog version 13.0 (devel)\n     0.00ns INFO     cocotb                             Running tests with cocotb v1.7.2 from /headless/miniforge3/envs/cocotb/lib/python3.11/site-packages/cocotb\n     0.00ns INFO     cocotb                             Seeding Python random module with 1767944425\n     0.00ns INFO     cocotb.regression                  Found test test.test_matmul.test_matadd\n     0.00ns INFO     cocotb.regression                  running test_matadd (1/1)\n12300001.00ns INFO     cocotb.regression                  test_matadd passed\n12300001.00ns INFO     cocotb.regression                  **************************************************************************************\n                                                          ** TEST                          STATUS  SIM TIME (ns)  REAL TIME (s)  RATIO (ns/s) **\n                                                          **************************************************************************************\n                                                          ** test.test_matmul.test_matadd   PASS    12300001.00         150.74      81596.40  **\n                                                          **************************************************************************************\n                                                          ** TESTS=1 PASS=1 FAIL=0 SKIP=0           12300001.00         151.43      81224.19  **\n                                                          **************************************************************************************\n\nTaking about \\(12\\) ms in sim time and about \\(2.5\\) min for the computation.\nDATA MEMORY\n+----------------+\n| Addr | Data     |\n+----------------+\n| 0    | 1        |\n| 1    | 2        |\n| 2    | 3        |\n| 3    | 4        |\n| 4    | 1        |\n| 5    | 2        |\n| 6    | 3        |\n| 7    | 4        |\n| 8    | 0        |\n| 9    | 0        |\n| 10   | 0        |\n| 11   | 0        |\n+----------------+\n\n... BUNCH OF LOGS OF SO MANY CYCLES ...\nCompleted in 491 cycles\n\nDATA MEMORY\n+----------------+\n| Addr | Data     |\n+----------------+\n| 0    | 1        |\n| 1    | 2        |\n| 2    | 3        |\n| 3    | 4        |\n| 4    | 1        |\n| 5    | 2        |\n| 6    | 3        |\n| 7    | 4        |\n| 8    | 7        |\n| 9    | 10       |\n| 10   | 15       |\n| 11   | 22       |\n+----------------+\nSo it definitely worked. But is it supposed to be this slow? It only took 491 cycles.\n\n\nThe Simulation\nThe simulation uses cocotb. The memories are implemented as python classes, but all this does is “change up the values”. The main simulation meat is here:\n    cycles = 0\n    while dut.done.value != 1:\n        data_memory.run()\n        program_memory.run()\n\n        await cocotb.triggers.ReadOnly()\n        format_cycle(dut, cycles, thread_id=1)\n        \n        await RisingEdge(dut.clk)\n        cycles += 1\n\nData/Program Memories\nData/program memories are single-cycle. The memory controller was needed to arbitrate concurrent thread read/write attempts.\n\n\n\nThe ISA\n\n\nCompilation\nThe project doesn’t come with its own compiler, but I think it’s some sort of personal ISA of the author.\nMax-astro made his own compiler for it in Scala Some other guy made one in Rust. I think Rust would be easier to use (has a conda feedstock)\n\n\n\nThe Hardware\n\nRegister File\n\n\n\n\n\n\nNote\n\n\n\nIn this context, thread=core, I guess. There is no concept of hyperthreading or software threads.\n\n\nEach thread has its own register file with additional read-only registers:\n\n`%blockIdx - register 1101\n%blockDim - register 1110\n%threadIdx - register 1111 this is heavily used by the code.\n\nThey’re declared like this\n            // Initialize read-only registers\n            registers[13] &lt;= 8'b0;              // %blockIdx\n            registers[14] &lt;= THREADS_PER_BLOCK; // %blockDim\n            registers[15] &lt;= THREAD_ID;         // %threadIdx\n\n\n\n\n\n\nWhy blockIdx &lt;= '0?\n\n\n\nThere is only one block in TinyGPU\n\n\n\n\nMemory Controller FSM\nThere are 4 memory channels (by default), each which has a single controller. The memory controller responds to valid assertions by consumers and then services them.\nThe consumers (compute cores) hold consumer_write_valid or consumer_read_valid until they’re given the ready. This makes sense per the usual treatment of ready-valid handshake.\n\n\n\n\n\nflowchart TD\n    IDLE --&gt;|consumer_read_valid && !channel_serving_customer| READ_WAITING\n    IDLE --&gt;|consumer_write_valid && !channel_serving_consumer| WRITE_WAITING\n    READ_WAITING --&gt;|mem_read_ready| READ_RELAYING\n    WRITE_WAITING --&gt;|mem_write_ready| WRITE_RELAYING\n    READ_RELAYING --&gt;|!consumer_read_valid| IDLE\n    WRITE_RELAYING --&gt;|!consumer_write_valid| IDLE"
  },
  {
    "objectID": "notes/Observer falls into the black hole, becomes Nostradamus.html",
    "href": "notes/Observer falls into the black hole, becomes Nostradamus.html",
    "title": "Observer falls into the black hole, becomes Nostradamus",
    "section": "",
    "text": "Someone on the inside of a black hole can observe the outside of the black hole\nThe observer falling through the event horizon can fall through it in finite local time\nSomeone watching from afar will see that the object falling into the event horizon takes infinite time.\nHence, the observer can watch the entire fate of the universe happen just as they fall into the event horizon?\n\n\nCaveats and Counters\n\nIf a photon falls into the event horizon later than the initial observer, then would it catch up to the observer?\n\nIf the observer is moving at less than \\(c\\), the photon will catch up.\nHence, this is not a counterargument. Photons about the outside WILL reach the eyes of the observer.\n\nThe data becomes impossibly redshifted as it falls into your eyes.\n\nIf you’re not yet through the event horizon, it won’t be “impossibly” redshifted. It’s red, but still observably so.\n\n\n\n\nThe true crowdsourced answer\n\nThe object doesn’t take infinite time to fall into a black hole as observed from outside. However, the light left behind by the object takes basically forever to show that.\nIt is impossible for the object’s light to show it crossing through the event horizon.\n\n\nIf the observer is moving at less than \\(c\\), the photon will catch up.\n\nThis is false. The photon cannot catch up says some reddit user.\nWe’ll need some treatment of the Kruzskal-Szekerez coordinates to confirm if this is the case."
  },
  {
    "objectID": "notes/Metrics and Manifolds.html",
    "href": "notes/Metrics and Manifolds.html",
    "title": "Metrics and Manifolds",
    "section": "",
    "text": "Manifolds have many points \\(p\\) with some coordinates.\nManifold definition To be a manifold, for any point \\(p\\), there is an open set of points \\(U\\) such that \\(\\phi(U)\\) is an open set in \\(R^n\\) \\(\\phi\\) has to be a continuous bijection and so \\(\\phi(U)\\) has to be a joint, connected set\nExtra info (Set theorem): &gt; *If \\(A\\) is a collection of connected sets and \\(\\cap A\\) is non-empty, &gt; then \\(\\cup A\\) is connected.\n\n\nFrom the definition of an open set in Euclidean space\nfor any point in an open set you can draw a ball that stays entirely within the set\na ball is a set within \\(\\epsilon\\) distance of some center point in this case, the center is the point \\(p\\)\n\n*but open sets aren’t necessarily continuous. Take for example the set of points \\(f(x)=1/x\\) where \\(x \\in (0,1)\\) This is an open set but it’s discontinuous."
  },
  {
    "objectID": "notes/Metrics and Manifolds.html#open-sets-are-unions-of-balls",
    "href": "notes/Metrics and Manifolds.html#open-sets-are-unions-of-balls",
    "title": "Metrics and Manifolds",
    "section": "",
    "text": "From the definition of an open set in Euclidean space\nfor any point in an open set you can draw a ball that stays entirely within the set\na ball is a set within \\(\\epsilon\\) distance of some center point in this case, the center is the point \\(p\\)\n\n*but open sets aren’t necessarily continuous. Take for example the set of points \\(f(x)=1/x\\) where \\(x \\in (0,1)\\) This is an open set but it’s discontinuous."
  },
  {
    "objectID": "notes/Metrics and Manifolds.html#length-of-a-tropic",
    "href": "notes/Metrics and Manifolds.html#length-of-a-tropic",
    "title": "Metrics and Manifolds",
    "section": "Length of a tropic",
    "text": "Length of a tropic\nFor example, then, we can use this metric to compute the length of a tropic at azimuth \\(\\theta_0\\).\n\\[ds^2=R^2d\\theta^2+R^2\\sin^2{\\theta} d\\phi^2\\] Since \\(d\\theta=0\\) (constant) then \\[ds=R\\sin{\\theta_0}d\\phi\\] \\[L=\\int ds\\] We also need to apply the limits of the curve so… \\[L=\\int_0^{2\\pi}{R\\sin{\\theta_0}d\\phi}\\] \\[L=2\\pi R\\sin{\\theta_0}\\] Which is something we’ve always known in high school geometry anyway."
  },
  {
    "objectID": "notes/Metrics and Manifolds.html#length-of-a-circular-fence-on-the-earth",
    "href": "notes/Metrics and Manifolds.html#length-of-a-circular-fence-on-the-earth",
    "title": "Metrics and Manifolds",
    "section": "Length of a circular fence on the earth",
    "text": "Length of a circular fence on the earth\n\n\n\n\n\n\nAnecdote\n\n\n\nOnce upon a time, there was a millionaire who challenged 3 of his farmers in exchange for his entire fortune. “I shall give my entire fortune to the one who can make a fence that covers the largest possible area!” and so all of the farmers put in their best efforts. The first farmer made a square fence. The second farmer scoffed at him. He knew that for any 2D shape, the highest area-to-perimeter belongs to the circle. And so, the second farmer built the largest circular fence he could with the available material. He then saw the third farmer’s fence, which was also circular, but it was extremely small “You clearly lose. Why didn’t you try building it larger?” The third farmer then said: “It’s simple.” The third farmer steps into the circle of his fence. “This part is on the outside of my fence.”\n\n\nNow how is this relevant? It’s not. I just remembered that story. Anyway, let’s try to draw a circle around the Philippines. What would be the true perimeter of that circle? Since it lies on a sphere, it has to be smaller than it would be on a flat plane. This is because the radius \\(r\\) of the sphere as measured by an ant is larger than the actual radius (it’s actually an arc length for us gods).\nAnd it’s at this point that I realize, this is the same problem as that of the length of the tropic. We just changed the coordinates (center is somewhere in the Philippines instead of one of the poles).\nSo let’s change the problem to that of a square."
  },
  {
    "objectID": "notes/Metrics and Manifolds.html#length-of-the-square-fence-on-earth",
    "href": "notes/Metrics and Manifolds.html#length-of-the-square-fence-on-earth",
    "title": "Metrics and Manifolds",
    "section": "Length of the square fence on Earth",
    "text": "Length of the square fence on Earth\nFor simplicity, let’s build a massive square fence spanning \\(\\pi/4\\) rad around the earth all around.\nSo then we can just sum up the 4 arc lengths that produce this.\nWe have four sections:\n\n\\(\\theta \\in [0,\\pi/4], \\phi=0\\)\n\\(\\phi \\in [0,\\pi/4], \\theta=\\pi/4\\)\n\\(\\phi \\in [0,\\pi/4], \\theta=0\\)\n\\(\\theta \\in [0,\\pi/4], \\phi=\\pi/4\\)\n\n\nNote: \\(\\theta\\) is the polar angle, the colatitude.\n\nSo then, we have 4 different integrals. For 2 and 3, we have \\(\\frac{\\pi}{4} R \\frac{\\sqrt{2}}{2}\\) and \\(0\\) For 1 and 4, we have that \\(ds^2=R^2d\\theta^2+R^2\\sin^2{\\theta}d\\phi^2\\) \\(ds^2=R^2d\\theta^2\\) \\(ds=Rd\\theta\\) \\(L=\\int_0^{\\pi/4}{Rd\\theta}\\) \\(L=\\frac{\\pi}{4}R\\) regardless of \\(\\phi\\).\nso we get \\[L=\\frac{\\pi}{4}R(\\frac{\\sqrt{2}}{2}+2)\\] &gt; Funnily, the zero makes sense because if \\(\\theta\\) is a polar angle, the square fence we made is actually a triangle where one of the vertices is at the pole"
  },
  {
    "objectID": "notes/LLMs are central nervous systems.html",
    "href": "notes/LLMs are central nervous systems.html",
    "title": "LLMs can be central nervous systems?",
    "section": "",
    "text": "Apparently AI is shifting towards trying to use LLMs in different manners like"
  },
  {
    "objectID": "notes/LLMs are central nervous systems.html#approaches-to-stateful-llm",
    "href": "notes/LLMs are central nervous systems.html#approaches-to-stateful-llm",
    "title": "LLMs can be central nervous systems?",
    "section": "Approaches to Stateful LLM",
    "text": "Approaches to Stateful LLM\nWith some effort to arrange “evolutionarily”:\n\nNeuro-sama (some guy named Vedal) - Probably a text file used with the context window.\nRecurrent Memory Transformers (NeurIPS 2022) - Add special memory tokens to the input sequence and output sequence. Then, eat your own output.\n\nKind of like Ghajini/Memento in a way. You don’t remember anything, but you do take notes.\n\nStructured State Space Models (various) - compressed latent space representation memory, but still explicit memory.\n\nMamba - They turned SSM parameters dependent on input \\(P(x)\\).\n\nTitans (Google 2024) - something like a high learning rate module (contextual memory) turning the medium-term context into parameters. Might be the most biological of all of these?\n\n\n\n\nTitans’ Memory as a Context (MAC)\n\n\n\n\n\nVinyals, Oriol, et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” nature 575.7782 (2019): 350-354.\nTitans’ Memory as a Context (MAC)"
  },
  {
    "objectID": "notes/Interesting Quantization Observations.html",
    "href": "notes/Interesting Quantization Observations.html",
    "title": "Interesting Quantization Observations",
    "section": "",
    "text": "Spreads, RMSE and SNR vs Bits\nWith playing around with code from github.com/Lawrence-lugs/hwacc_design_garage\nwDimX = 8\nwDimY = 128\nxBatches = 10\nwBits = 8\nxBits = 8\nseed = 0\noutBits = 8\n# np.random.seed(seed)\n\nwShape = (wDimX, wDimY)\nxShape = (xBatches, wDimY)\n\nw = np.random.uniform(-1, 1, wShape)\nx = np.random.uniform(-1, 1, xShape)\nwx = w @ x.T\n\n# Calculate the output scale\nwx_qtensor = quant.quantized_tensor(\n    real_values=wx,\n    precision=outBits,\n    mode='maxmin'\n)\n\n# Set numpy printing to at most 3 significant digits\nnp.set_printoptions(precision=3)\n\n# print('w\\n',w)\n# print('x\\n',x)\n# print('wx\\n',wx)\n\n# Quantize the weights and input\nwQ = quant.quantized_tensor(\n    real_values=w,\n    precision=wBits,\n    mode='maxmin'\n)\nxQ = quant.quantized_tensor(\n    real_values=x,\n    precision=xBits,\n    mode='maxmin'\n)\nwQxQ_qvals_t = wQ.quantized_values @ xQ.quantized_values.T\n\nout_scale = wx_qtensor.scale\n\nwQxQ_t = quant.quantized_tensor(\n    quantized_values=wQxQ_qvals_t,\n    scale=out_scale,\n    zero_point = 0\n)\n\nwQxQ = quant.scaling_quantized_matmul(\n    wQ, xQ, outBits, internalPrecision=16, out_scale = out_scale\n)\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(wx.flatten(), wQxQ.fake_quantized_values.flatten())\n\nwx, wQxQ.fake_quantized_values, wQxQ.quantized_values\nChanging wBits, xBits and outBits gives some interesting tradeoffs\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nThere should be some mathematical expected SNR based on quantization that corresponds to these.\n\n\nWeight Quantization\nweights need at least 2 bits to be viable in the ternary form\nthey don’t seem to quantize into -2 because it’s symmetric over the zero point\n\n\n\nimage.png\n\n\nWith 1-bit quantization, only zeros appear because the zero point is 0, yet there’s only 1 bit to represent everything.\n\nADC Outputs and Encoding\n00 0000 0000\n98 7654 3210\n\n\n\nimage.png\nimage.png\nimage.png\nimage.png\nimage.png\nimage.png\nimage.png\nimage.png\nimage.png"
  },
  {
    "objectID": "notes/Humans, Information and LLMs.html",
    "href": "notes/Humans, Information and LLMs.html",
    "title": "Information theory says humans reduce information, but that’s ok",
    "section": "",
    "text": "Summary\n\n\n\n\nHumans as computing agents transform raw information (say, NIRSpec data) into records (as in, conclusions from data).\nRecords in technically contain less information than the raw information. It’s more ordered, so via information theory less there’s not much more you can conclude.\nInformation in the universe tends to a maximum because it’s proportional to entropy. However, it gets harder to extract useful information in the way humans and computers do.\nWe appreciate records/conclusions made by humans more than raw data (say, in bits) because they increase the mutual information between the world and a record.\n\n\n\n\n\n\nprocessing pipeline\n\n\nComputation (or information transformation) can’t create more information than what’s already available. However, it can make latent information explicit.\nIn physics, mass-energy gives a bound on the amount of information an object is allowed to possess (Bekenstein Bound). However, this doesn’t mean that objects with the same energy contain the same amount of information.\nStringy molecule soup vs DNA of the same temperature “basically” has a similar macrostate. Stringy molecule soup has more entropy and hence more information. However, DNA contains much less but more meaningful information.\n\n\n\nprocessing pipeline"
  },
  {
    "objectID": "notes/Differential Equations to FEM.html",
    "href": "notes/Differential Equations to FEM.html",
    "title": "Differential Equations to FEM",
    "section": "",
    "text": "Important\n\n\n\n\nPeople have definitely come up with differential equations long before computers were around\nDifferential equations are sometimes so hard there aren’t any analytical solutions\nComputers were probably a lifesaver when they came around\nHow did people start using computers and developing numeric algorithms to solve differential equations?\nI want to try it in C\nThe divergence operator \\(\\vec\\nabla\\) looks and sounds as cool as ever"
  },
  {
    "objectID": "notes/Differential Equations to FEM.html#solving-pde-with-finite-elements-in-general",
    "href": "notes/Differential Equations to FEM.html#solving-pde-with-finite-elements-in-general",
    "title": "Differential Equations to FEM",
    "section": "Solving PDE with Finite Elements in General",
    "text": "Solving PDE with Finite Elements in General\nLet’s say we have a general residual PDE \\(R\\) of order \\(N\\) we have \\[\\int_\\Omega dx R\\cdot v=0\\] we are guaranteed to be able to split the order between \\(R\\) and \\(v\\) using IBP getting order \\(N/2\\). If solving more than a second order PDE (like in the beam equation), we’re supposed to do mixed finite elements where we split the original differential equation into coupled differential equations, like in earlier.\nOnce we have a first-order weak form, we apply the FEM hat function formulation. This essentially eliminates space as a variable by performing the integrals over the easy-to-work-with hat functions \\(\\phi\\) and placing them into the \\(\\mathbf{M}\\) and \\(\\mathbf{K}\\) matrices.\n\\[u(x,t)=\\sum_{j=1}^{N}u_j(t)\\phi_j(x)\\] In the end, we get a system of ODEs that also depend on the length of the finite element hat \\(h\\) (characteristic mesh size) \\[\\mathbf{M}\\frac{\\vec{d^nu}}{dt^n}+\\mathbf{K}\\vec{u}=\\mathbf{F}\\] ::: {.callout-warning title=“Limitations”} Not all forms look like that. That form is limited to\n\nLinear PDE\nTime dependence is only via time derivative\nNo auxiliary fields or damping (so no wind in the ballistics equations either) :::"
  },
  {
    "objectID": "notes/Adding an LLM to AlphaStar's Brain.html",
    "href": "notes/Adding an LLM to AlphaStar's Brain.html",
    "title": "Adding an LLM to AlphaStar’s Brain",
    "section": "",
    "text": "Apparently AI is shifting towards trying to use LLMs in different manners like"
  },
  {
    "objectID": "notes/Adding an LLM to AlphaStar's Brain.html#approaches-to-stateful-llm",
    "href": "notes/Adding an LLM to AlphaStar's Brain.html#approaches-to-stateful-llm",
    "title": "Adding an LLM to AlphaStar’s Brain",
    "section": "Approaches to Stateful LLM",
    "text": "Approaches to Stateful LLM\nWith some effort to arrange “evolutionarily”:\n\nNeuro-sama (some guy named Vedal) - Probably a text file used with the context window.\nRecurrent Memory Transformers (NeurIPS 2022) - Add special memory tokens to the input sequence and output sequence. Then, eat your own output.\n\nKind of like Ghajini/Memento in a way. You don’t remember anything, but you do take notes.\n\nStructured State Space Models (various) - compressed latent space representation memory, but still explicit memory.\n\nMamba - They turned SSM parameters dependent on input \\(P(x)\\).\n\nTitans (Google 2024) - something like a high learning rate module (contextual memory) turning the medium-term context into parameters. Might be the most biological of all of these?\n\nThey still just concatenate the learnable parameters with the input. Difference with RMT is that these are learnable parameters, not previous outputs.\nWhy not do both? Humans do both as creatures.\n\n\n\nTitans’ Memory as a Context (MAC) \n\n\nThe RMT. Quite a beautiful hack, actually. \n\nSo then, we can try to use the above to improve on AlphaStar.\nUnfortunately, I do not possess the X100 machines to do work on LLMs nor AlphaStar. Or so goes my excuse."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lawrence Quizon",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n     Google Scholar\n  \n\n      \nResearcher of everything! Formally, an IC design researcher in University of the Philippines Diliman Microlab where I recently obtained my Master’s degree in August 2025.\nMy primary research interest is in the creation of efficient computational systems in software and hardware for the advancement of both engineering (low power computation on the edge, brain-computer interfaces) and sciences (device physics, emergent dynamics).\n\n    \n    \n  \n\n\n\n\nSecondary Interests\n\nOpen source hardware everything. Why can’t we have massive Github portfolios of random hardware? (We can)\nThe Linux Kernel as a representative of the missing region between microarchitecture and user level.\nThe very very secret GPU architectures\nPractical Applications of Abstract Math\nTrying to understand AdS/CFT by Maldacena. Currently still in GR and QFT.\nDifferential equations and numerical methods have and always will be the most exciting application of computing.\nLearning an NVIM workflow, primarily because I’m tired of using VNC.\n\n\n\nRecent Pages\nNotes below are sometimes attempts on active recall, sometimes just literal pastebins while working on something.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nSolving Ballistics Equation with RK4 in C\n\n\nJan 12, 2026\n\n\n\n\nThe Generalized Stokes’ Theorem\n\n\nJan 11, 2026\n\n\n\n\nDifferential Equations to FEM\n\n\nJan 10, 2026\n\n\n\n\nRings and Fields\n\n\nJan 3, 2026\n\n\n\n\nMetrics and Manifolds\n\n\nDec 26, 2025\n\n\n\n\nObserver falls into the black hole, becomes Nostradamus\n\n\nDec 18, 2025\n\n\n\n\nSetting up an OS PDK\n\n\nDec 17, 2025\n\n\n\n\nDerivation of the Lorentz Transformation\n\n\nDec 5, 2025\n\n\n\n\nWavefunctions, Bras, Kets\n\n\nDec 4, 2025\n\n\n\n\nRunning TinyGPU by Adam Maj\n\n\nNov 28, 2025\n\n\n\n\nFrequency Response of CE Amplifier\n\n\nNov 28, 2025\n\n\n\n\nThe Linux Page Fault Handler\n\n\nNov 24, 2025\n\n\n\n\nTransistor Capacitances\n\n\nNov 19, 2025\n\n\n\n\nAI for Learning and Feeling stupid organically\n\n\nOct 20, 2025\n\n\n\n\nLLMs can be central nervous systems?\n\n\nOct 3, 2025\n\n\n\n\nAdding an LLM to AlphaStar’s Brain\n\n\nOct 2, 2025\n\n\n\n\nLagrangians, Hamiltonians\n\n\nOct 1, 2025\n\n\n\n\nInformation theory says humans reduce information, but that’s ok\n\n\nSep 23, 2025\n\n\n\n\nGeneralized Eigenvalues in FEM Formulations\n\n\nSep 18, 2025\n\n\n\n\nHyperCoreX Code Structure\n\n\nMar 18, 2025\n\n\n\n\nMethods in Quantization for TinyML Frameworks\n\n\nMar 11, 2025\n\n\n\n\nInteresting Quantization Observations\n\n\nJan 17, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/AI for Learning and Feeling stupid organically.html",
    "href": "notes/AI for Learning and Feeling stupid organically.html",
    "title": "AI for Learning and Feeling stupid organically",
    "section": "",
    "text": "Fundamental Question\n\n\n\nIs there a point in building something from scratch and getting stuck on your own?\nAny studies on how to make this AI-assisted learning workflow effective?\nThere has to be a balance to strike between not wasting time getting stuck but also still encountering meaningful “stuck” for your brain to attempt recall and “save information”.\n\n\n\nThe workflow\nLet’s start from the conclusion of this study: the AI-assisted workflow.\n\nStart by mapping out a path to the answer.\n\nDo you know all the tools and concepts you’ll need? Then proceed and don’t look at the AI.\nIf you don’t know the general process, then it’s time to ask.\n\nAsk, but do not look at the answer.\n\nWhen asking AI for a derivation, look at the relevant concept headers. Then do it yourself.\nWhen trying to code: ask, skim VERY quickly, then code it.\nThis scheme likely allows you to know if you can’t do it yourself, but also allows you to do it yourself anyway.\n\nGet stuck: either glimpse the answer or prompt for a very specific part.\n\n\n\nGetting Stuck Matters\n\nAs a researcher, if you don’t feel stupid everyday, you’re doing something wrong.\n\nVague recollection of some meme or anecdote. I don’t remember which.\n\n\n\nDesirable difficulty - introducing difficulties in the learning process improves long-term retention.\nIllusion of competence - you cannot generate neural pathways to the solution by reading the solution.\n\n\n\nWhy against AI for learning?\nThere are not yet cognitive psychology studies on long/medium term learning retention with AI\nHowever, personal observation (may be biased) seems like it does affect problem solving skills in the long term.\nHence, AI is likely detrimental to retention.\nAdditionally, we can predict the results of using AI by arguing using the Generation effect.\n\n\n\n\n\n\nThe Generation effect\n\n\n\nOn 5 experiments with 96 undergraduates, they compared memory for words that were generated by the subjects themselves vs the same words when presented to be read.\nRetention is higher for self-generated words.\nBut this is a mundane memory task. What if it was a problem solving or design task?\n\n\nIn Bjork & Bjork 2011, they coined “Desirable Difficulties”. Particularly,\n\n“Basically, any time that you, as a learner, look up an answer or have somebody tell or show you something that you could, drawing on current cues and your past knowledge, generate instead, you rob yourself of a powerful learning opportunity”\n\nSo if you give up on trying to remember and ask for help, you’ve lost? It may not be so gray.\nFor example, when trying to independently derive some sort of conclusion, instead of being unable to recall something, it may not be there at all. You’re trying to squeeze an empty sponge. For example, a mathematical derivation requires some obscure Ring theorem- but you’ve never done Abstract Algebra.\nEnter productive failure.\n\n\n\n\n\n\nProductive Failure\n\n\n\nIf you lack the prerequisite concept (e.g., you don’t know what functions are around to do a specific type of work), your struggle is Unproductive Failure. You are not “generating”; you are just guessing."
  },
  {
    "objectID": "notes/derivationOfTheLorentzTransformation.html",
    "href": "notes/derivationOfTheLorentzTransformation.html",
    "title": "Derivation of the Lorentz Transformation",
    "section": "",
    "text": "The observation goes as follows:\n\n\nThe speed of light is \\(c\\) in all inertial frames of reference\nHence, time and space must warp (transform) in order to make movement (velocity) slower when an object is moving relativistically.\n\n\nHence, consider a 1D space where two objects A and B are moving relative to each other at a velocity \\(v\\). Both objects have a pair of mirror separated by a distance \\(x\\) with a laser bouncing back and forth.\nFrom object A, the speed of its own laser is \\(c\\) and space is warped by a factor of \\(1\\) (unwarped).\nFrom object B, the observed speed of A’s laser has to be \\(c\\).\nSo then from the perspective of B, the laser has to catch up to the A mirror moving at \\(v\\). The laser would have then been travelling at \\(c-v\\) relative to the A mirror and would have taken \\(\\frac{x}{c-v}\\) time to get there, as opposed to \\(x/c\\).\nThen, for the return trip, the laser would take \\(x/(c+v)\\) time. So then the total time for the laser to bounce has become:\n\\[t'=\\frac{x}{c-v}+\\frac{x}{c+v}=x(\\frac{1}{c-v}+\\frac{1}{c+v})=x(\\frac{2c}{c^2-v^2})\\]\nSolving for the time dilation factor,\n\\[t'/t=[x(\\frac{2c}{c^2-v^2})]\\frac{c}{2x}=\\frac{c^2}{c^2-v^2}=\\frac{1}{1-\\frac{v^2}{c^2}}\\]\n\nThe perpendicular mode\nLet’s then assume the same situation but the mirrors pointed perpendicular to the direction of moment.\nThen, the laser will have to travel the hypotenuse of a triangle with side lengths \\(x\\) and the length object A has travelled in the time it takes for the laser to reach the mirror \\(vt'/2\\).\nThe path of the laser after it’s been reflected is symmetric to this. Hence, the time dilation factor should be the same for half the time.\nSo then, the time\n\\[t_h'=\\frac{\\sqrt{x^2+vt_h'^2}}{c}\\]\n\\[t_h'^2=\\frac{x^2+v^2t_h'^2}{c^2}\\]\n\\[c^2t_h'^2-v^2t_h'^2-x^2=0\\]\n\\[t_h'^2(c^2-v^2)=x^2\\]\n\\[t_h'^2=\\frac{x^2}{c^2-v^2}\\]\n\\[t_h'^2=\\frac{x^2}{c^2}\\frac{1}{1-\\frac{v^2}{c^2}}\\]\n\\[t_h'^2/t_h^2=\\frac{1}{1-v^2/c^2}\\]\n\\[l=\\frac{1}{\\sqrt{1-\\frac{v^2}{c^2}}}\\]\n\n\nLength Contraction\nSo then, we find that the time it takes for the laser to finish a cycle is different in these cases.\nThis indicates that space should have warped one of the cases so that both clocks still finish at the same time.\n\\[\\frac{x^2}{c^2-v^2}=x'^2\\frac{c^2}{(c^2-v^2)^2}\\]\n\\[\\frac{x'}{x}=\\sqrt\\frac{c^2-v^2}{c^2}\\]\n\\[\\frac{x'}{x}=\\sqrt{1-\\frac{v^2}{c^2}}&lt;1\\]\nand we find that length contracts in the parallel case."
  },
  {
    "objectID": "notes/Generalized Eigenvalues in FEM Formulations.html",
    "href": "notes/Generalized Eigenvalues in FEM Formulations.html",
    "title": "Generalized Eigenvalues in FEM Formulations",
    "section": "",
    "text": "Generalized Eigenvalue Problem\nThe eigenvectors and eigenvalues of \\(\\mathbf{M}\\) and \\(\\mathbf{K}\\) can significantly simplify solutions of the FEM problem.\nDecomposing the solution \\(u(x,t)\\) into a linear combination of eigenvalues and eigenvectors allows to solve only the \\(N\\) (like 20 or so) most important modalities. The rest of the eigenvectors contribute so little to \\(u(x,t)\\) that numeric error outscales it, so they’re not really necessary.\nWe can look at the unforced FEM formulation of the heat equation for this: \\[\\mathbf{M}\\vec{\\frac{du}{dt}}+\\mathbf{K}\\vec{u}=0\\] If we can find the eigenvector \\(\\vec{v}\\) of \\(\\mathbf{K}\\) and \\(\\mathbf{M}\\) with eigenvalue \\(\\lambda\\), we can just do: Let \\(\\vec{u}=\\vec{v}e^{-\\lambda t}\\) (decay). Then, \\[-\\mathbf{M}\\lambda\\vec{u}+\\mathbf{K}\\vec{u}=0\\] \\[-\\mathbf{M}\\lambda\\vec{v}e^{-\\lambda t}+\\mathbf{K}\\vec{v}e^{-\\lambda t}=0\\] \\[-\\mathbf{M}\\lambda\\vec{v}+\\mathbf{K}\\vec{v}=0\\] and we get that the \\(\\lambda\\) are the eigenvalues in a generalized eigenvalue problem.\nThen, we can use this in some Fourier or Laplace magic (in this case, laplace magic because the time-varying forms called \\(\\xi(t)=e^{-\\lambda t}\\) are decay forms)\nWe can project the solution into a basis of a bunch of discrete \\(\\lambda\\) time constants and add up the solutions for each of those.\n\n\n\n\n\n\nThe Spectral Theorem\n\n\n\nIn fact, the spectral theorem gives us that if \\(K\\) and \\(M\\) are symmetric positive-definite matrices, we can always get real \\(\\lambda\\) and that the eigenvectors \\(v_1,v_2,...,v_3\\) form a basis. This means we can project the true solution into the basis.\n\n\nSo then, the exact solution becomes a linear combination of eigenvectors: \\[u(t)=\\sum^{N}_{i=1} c_iv_ie^{-\\lambda_it}\\]\n\n\nGeneralized eigenvalues with a force term\nWith a force term, we get something like \\[-\\mathbf{M}\\lambda\\vec{v}e^{-\\lambda t}+\\mathbf{K}\\vec{v}e^{-\\lambda t}=\\mathbf{F}\\] In this case, the time term is incorrect (the forcing term prevents it from just being a decay, making the “guess” incorrect)\nSo then, we just avoid guessing a time term completely.\nLet \\(\\vec{u}=\\vec{v_i}\\xi_i(t)\\)\nThen we can use this in the FEM formulation \\[\\mathbf{M} \\left( \\sum_{i=1}^N \\dot{\\xi}_i(t) \\vec{v}_i \\right) + \\mathbf{K} \\left( \\sum_{i=1}^N \\xi_i(t) \\vec{v}_i \\right) = \\mathbf{F}(t)\\] To isolate the solution for a specific mode, \\(v_i\\), we multiply with the ket form \\(v_j^T\\) to “precipitate out” the important terms. \\[\\vec{v}_j^T \\mathbf{M} \\left( \\sum \\dot{\\xi}_i \\vec{v}_i \\right) + \\vec{v}_j^T \\mathbf{K} \\left( \\sum \\xi_i \\vec{v}_i \\right) = \\vec{v}_j^T \\mathbf{F}(t)\\] That is, important terms being only the ones with \\(i=j\\). For the other terms, \\[v_j^T \\mathbf{M} v_i=0 \\text{ if }i \\neq j \\text{ (orthogonal basis)}\\] \\[v_j^T \\mathbf{K} v_i=0 \\text{ if }i \\neq j \\text{ (orthogonal basis)}\\] We’re left with the \\(\\xi_i(t)\\) terms mixed with a simple scalar form. \\[\\vec{v}_j^T \\mathbf{M} \\vec{v}_j\\dot{\\xi}_j(t) + \\vec{v}_j^T \\mathbf{K} \\vec{v}_j\\xi_j(t)=\\vec{v}_j^T\\mathbf{F}(t)\\] We call these terms “modal mass, modal stiffness, and modal force”: \\[m_j \\dot{\\xi}_j(t) + k_j \\xi_j(t) = f_j(t)\\]\n\n\n\n\n\n\nTip\n\n\n\nInterestingly, these terms resemble the ket forms in quantum mechanics \\[\\bra{v}\\mathbf{M}\\ket{v}, \\bra{v}\\mathbf{K}\\ket{v}, \\bra{v}\\mathbf{F}\\] Apparetly \\(\\bra{v}\\mathbf{M}\\ket{v}\\) is called the kinetic energy norm and \\(\\bra{v}\\mathbf{K}\\ket{v}\\) is called the Strain Energy\n\n\nNow we have a simply solvable calculus problem: \\[\\frac{d\\xi_j}{dt}+\\lambda_j\\xi_j=\\frac{f_j(t)}{m_j}\\] whose solution is \\[\\xi_j(t)=\\xi_j(0)e^{-\\lambda_j t}+\\int_0^td\\tau e^{-\\lambda_j (t-\\tau)} \\frac{f_j(\\tau)}{m_j}\\]\nThe thing is, we have to do that for every single modality we’re accounting for.\nMaybe the good news is that solving for the \\(\\xi_j\\) form once probably makes it trivial to get all the others (\\(f_j\\) will likely rarely change in form per \\(j\\), more like only scaling)\n\n\n\n\n\n\nFor second-order FEM formulas\n\n\n\nFor things like the beam equation, for example, the solution turns out to be \\[\\ddot{\\xi} + \\omega_0^2 \\xi = f(t)\\] \\[\\xi(t) = \\int_0^t \\frac{1}{\\omega_0} \\sin(\\omega_0(t-\\tau)) f(\\tau) d\\tau\\]\n\n\n\n\nHow to solve a generalized eigenvalue problem?\nStarting from \\[-\\mathbf{M}\\lambda\\vec{v}+\\mathbf{K}\\vec{v}=0\\] we still need to solve for the \\(\\lambda_j\\)s and the \\(\\vec{v_j}\\)s.\nWe use the Lanczos algorithm for this (kind of like Givens rotations etc. algorithms for QR decomposition.)\n\n\n\n\n\n\nNote\n\n\n\nThis is nice because the Lanczos algorithm can give the eigenvalues in order of importants (like PCA). That’s because it usually gives it in order of highest to lowest (like most linalg iterative algorithms) but if you do shift-and-invert you can make the lowest eigenvalues the highest.\n\n\n\n\nWhere is this used?\nOnly used for problems with nice well-defined structures whose properties \\(\\mathbf{M}\\) and \\(\\mathbf{K}\\) do not change over the course of the simulation.\nProblems where it’s applicable:\n\nTall building (doesn’t really change behavior until it breaks)\nPCB vibration\nDrivetrains and gears\nLaminar flow (apparently you just have the Stokes Equation for this)\n\nProblems where it’s inapplicable:\n\nTurbulent flow (you have to use Navier-Stokes, which has an advection \\((\\vec{v} \\cdot \\nabla) \\vec{v}\\) term that will appear as \\(\\mathbf{K}(v)\\). Stiffness is a function of \\(v\\). For these, we have to directly solve the FEM form.\n\n\n\nNotes\n\n\n\n\n\n\nNote\n\n\n\nThe eigenvalues \\(\\lambda_i\\) are not like harmonics- they can be randomly spaced in between each other. \\(\\lambda_2\\) might be \\(1.43 \\times \\lambda_1\\). \\(\\lambda_3\\) might be \\(2.11 \\times \\lambda_1\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn practice, we choose some “Nyquist maximum” frequency at which to stop caring about the solution. So then we solve for all \\(\\lambda_n &lt; 1Grads\\) or \\(\\omega_n &lt; 1Grads\\) if we expect the forcing function \\(f\\).\n\n\n\n\n\n\n\n\nCool thing:\n\n\n\n\\(\\vec{v}_j^T\\mathbf{F}(t)\\) is literally use projecting the “simulation stimulus force” onto the eigenvector basis. So then putting specific modalities of vibration or decay in \\(F\\) would allow us to simulate resonance type things. We’re not turning \\(F\\) into its fourier transform. Rather, we’re asking how much does \\(F\\) looks like \\(v_1\\)? How much does it look like \\(v_2\\)? etc.\n\n\n\n\n“Test coverage” and using eigenvalue-based FEM correctly:\n\n\n\n\n\n\nEffective Modal Inertia\n\n\n\nA parameter named “effective modal inertia” captures the “test coverage” of eigenvectors- capturing how much the current set of eigenvectors makes up the overall response to a stimulus \\(F\\).\nSimulating with a specific \\(F\\) like an earthquake you try to get up to 90% “test coverage” (effective modal inertia) on it.\nThen, you think of another \\(F\\) to test it out with. Say, supertyphoon winds, then you get 90% test coverage on that.\nIf you fail to find the \\(F\\) that breaks your thing, that’s unfortunate.\n\n\nI have an idea.\nI wonder if we can design \\(F\\) to be “constant white noise” in the “eigenvector basis” so that we can use one \\(F\\) to simulate all behaviors?\nThis is apparently called “Response Spectrum Analysis”. Commercial simulators have this feature, as it looks like in Google results.\n\nOk. Since we can disastrously miss resonance modes that can cause failure, let’s just go back to the direct FEM solution.\n\nFor this one, we’re limited instead by the \\(\\Delta t\\) used in the RK4 or the reverse euler solution."
  },
  {
    "objectID": "notes/HypercoreX  Structure.html",
    "href": "notes/HypercoreX  Structure.html",
    "title": "HyperCoreX Code Structure",
    "section": "",
    "text": "TODO: Add more commentary, since this is now a public note. \n\n\n\n\n\n\n\nNote\n\n\n\nHyperCoreX is Sir Ry’s General-purpose accelerator for Hyperdimensional Computing. We’d wanted to do something like this before.\n\n\n\nCSRs\nCORE_SET_ADDR_REG contains the core set of registers, including W0 Start Core and W0 Core Clear\nCORE_SET_REG_ADDR: begin\n  csr_rd_data = {\n                                                           // verilog_lint: waive-start line-length\n                                 {(CsrDataWidth-5){1'b0}}, // [31:5] -- Unused\n                                                     1'b0, //    [6] WO Core clear (generates pulse)\n  csr_set[CORE_SET_REG_ADDR][  CORE_SET_IMB_MUX_BIT_ADDR], //    [5] RW IMB MUX\n  csr_set[CORE_SET_REG_ADDR][4:CORE_SET_IMA_MUX_BIT_ADDR], //    [4:3] RW IMA MUX\n  csr_set[CORE_SET_REG_ADDR][ CORE_SET_SEQ_TEST_BIT_ADDR], //    [2] RW Sequential test\n                                               csr_busy_i, //    [1] RO Busy\n                                                     1'b0  //    [0] WO Start Core (generates pulse)\n                                                           // verilog_lint: waive-stop line-length\n  };\nTo actually start the accelerator, the start output is derived like so:\n    csr_start_o                = csr_write_req &&\n                                (csr_req_addr_i == CORE_SET_REG_ADDR) &&\n                                 csr_req_data_i[CORE_SET_START_CORE_BIT_ADDR];\nWhere csr_write_req =  csr_req_success & csr_req_write_i indicates a write request with a successful ready valid handshake\nand csr_req_success = csr_req_valid_i & csr_req_ready_o indicates a valid ready handshake\n\nIn the above, csr_req_data_i is read directly to find the trigger signal, not the CSR. Hence, the actual location in the CSR gets written the trigger bit, but it never actually gets used.\n\n\nThat means in the first trigger after nrst, it gets written to 1 and stays 1 forever.\n\nWhere does start go after that?\n\n\nOverall Architecture\n\nWhere does start go?\n\nTop level blocks:\n\ninst_decode\ninst_control\ncsr\nupdate_counter s — 2 of them\ndata_slicer\nitem_memory_top\nhv_encoder\nassoc_mem\n\nThe overall architecture seems to be an HDC vector processor with an instruction memory and instructions. The instructions don’t seem to come from outside, since there are no entrances for that.\n\n\nInstruction Handling\ninst_control contains the PC and a reg_file_1w1r (some replaceable IP, probably) which is the actual instruction memory\nmodule inst_control # (\n  parameter int unsigned RegAddrWidth       = 32,\n  parameter int unsigned InstMemDepth       = 128,\n  // Don't touch!\n  parameter int unsigned LoopNumStates      = 4,\n  parameter int unsigned InstLoopCountWidth = 10,\n  parameter int unsigned LoopNumWidth       = $clog2(LoopNumStates),\n  parameter int unsigned InstMemAddrWidth   = $clog2(InstMemDepth)\n)(\n  // Clocks and reset\n  input  logic                        clk_i,\n  input  logic                        rst_ni,\n  // Control signals\n  input  logic                        clr_i,\n  input  logic                        start_i,\n  input  logic                        stall_i,\n  output logic                        enable_o,\n  // Instruction update signals\n  input  logic                        inst_pc_reset_i,\n  input  logic                        inst_wr_mode_i,\n  input  logic [InstMemAddrWidth-1:0] inst_wr_addr_i,\n  input  logic                        inst_wr_addr_en_i,\n  input  logic [RegAddrWidth-1:0]     inst_wr_data_i,\n  input  logic                        inst_wr_data_en_i,\n  output logic [InstMemAddrWidth-1:0] inst_pc_o,\n  output logic [RegAddrWidth-1:0]     inst_rd_o,\n  // CSR control for loop control\n  input  logic [LoopNumWidth-1:0]     inst_loop_mode_i,\n  input  logic [InstMemAddrWidth-1:0] inst_loop_jump_addr1_i,\n  input  logic [InstMemAddrWidth-1:0] inst_loop_jump_addr2_i,\n  input  logic [InstMemAddrWidth-1:0] inst_loop_jump_addr3_i,\n  input  logic [InstMemAddrWidth-1:0] inst_loop_end_addr1_i,\n  input  logic [InstMemAddrWidth-1:0] inst_loop_end_addr2_i,\n  input  logic [InstMemAddrWidth-1:0] inst_loop_end_addr3_i,\n  input  logic [InstLoopCountWidth-1:0] inst_loop_count_addr1_i,\n  input  logic [InstLoopCountWidth-1:0] inst_loop_count_addr2_i,\n  input  logic [InstLoopCountWidth-1:0] inst_loop_count_addr3_i,\n  // Debug control signals\n  input  logic                        dbg_en_i,\n  input  logic [InstMemAddrWidth-1:0] dbg_addr_i\n);\ninstructions are written in via a passthrough the csr module (these outputs are received by inst_control\n    //---------------------------\n    // Instruction control settings\n    //---------------------------\n    csr_inst_ctrl_write_mode_o = csr_set[INST_CTRL_REG_ADDR][INST_CTRL_WRITE_MODE_BIT_ADDR];\n    csr_inst_ctrl_dbg_o        = csr_set[INST_CTRL_REG_ADDR][  INST_CTRL_DBG_MODE_BIT_ADDR];\n    csr_inst_ctrl_clr_o        =  csr_write_req &&\n                                 (csr_req_addr_i == INST_CTRL_REG_ADDR) &&\n                                  csr_req_data_i[INST_CTRL_INST_CLR_BIT_ADDR];\n    csr_inst_wr_addr_o         = csr_req_data_i[InstMemAddrWidth-1:0];\n    csr_inst_wr_addr_en_o      = csr_write_req && (csr_req_addr_i == INST_WRITE_ADDR_REG_ADDR);\n    csr_inst_wr_data_o         = csr_req_data_i;\n    csr_inst_wr_data_en_o      = csr_write_req && (csr_req_addr_i == INST_WRITE_DATA_REG_ADDR);\n    csr_inst_rddbg_addr_o      = csr_set[INST_RDDBG_ADDR_REG_ADDR];\nInside inst_control , start_i triggers the enable_core signal, which decides if things are done or not.\n  //---------------------------\n  // Enable core register\n  //---------------------------\n  always_ff @ (posedge clk_i or negedge rst_ni) begin\n    if (!rst_ni) begin\n      enable_core &lt;= 1'b0;\n    end else begin\n      if(start_i) begin\n        enable_core &lt;= 1'b1;\n      end else if (inst_loop_done) begin\n        enable_core &lt;= 1'b0;\n      end else begin\n        enable_core &lt;= enable_core;\n      end\n    end\n  end\n\nThis is genius. Who needs FSMs if you have a microcode loop????\n\n\nThe Stalls\n\nVery important: There is a hardware stall streaming into the controller from top, which allows the microcode loop to wait for ready valid handshakes and external flags\n\n\n  //---------------------------\n  // Stall Signal\n  //---------------------------\n  logic stall;\n  logic im_stall;\n  logic am_stall;\n  logic qhv_stall;\n\n  assign stall = im_stall || qhv_stall || am_stall;"
  },
  {
    "objectID": "notes/Lagrangians, Hamiltonians.html",
    "href": "notes/Lagrangians, Hamiltonians.html",
    "title": "Lagrangians, Hamiltonians",
    "section": "",
    "text": "The Lagrangian\n\\(\\frac{d}{dt}(\\frac{\\partial{L}}{\\partial{\\ddot{x}}})=-\\frac{\\partial{L}}{\\partial{x}}\\)\nis related to the Hamiltonian via phase space transformation\n\\(\\frac{dq}{dt}=\\frac{\\partial{H}}{\\partial{p}}\\)\n\\(\\frac{dp}{dt}=-\\frac{\\partial{H}}{\\partial{q}}\\)\n\nCoordinate Systems\nA standard coordinate system is like \\((q,\\dot q)\\) or \\((x,\\dot x)\\).\nIn Hamiltonian mechanics, this system turns into \\((x,mv)\\) or \\((p,q)\\) instead.\n\n\nMGH in Lagrangian\nLet’s consider the dynamic of a falling point object in Lagrangian mechanics.\nThen,\n\\(L= T-V\\)\nso the kinetic energy is\n\\(T=\\frac{1}{2}m\\dot x^2\\)\nand the potential energy is just\n\\(V=-mgx\\)\nso then\n\\(L=\\frac{1}{2}m\\dot x^2 + mgx\\)\nsolving the lagrangian gives us\n\\(m\\ddot x=-mg\\)\nor then that\n\\(a=-g\\)\nwhich is obvious.\n\n\nMGH in Hamiltonian Phase\nLet’s consider the dynamics of a falling point in Hamiltonian mechanics.\nThen, the Hamiltonian is\n\\(H=T+ V\\)\n\nGiven that T and V are written in terms of only P and Q\n\nThen if\n\\(T=\\frac{1}{2}mv^2\\)\nand\n\\(p=mv\\)\nthen\n\\(T=\\frac{1}{2}pv=\\frac{p^2}{2m}\\)\nand\n\\(V=-mgx=-mgq\\)\nwhich is already a valid Hamiltonian description\nso then\n\\(H=\\frac{p^2}{2m}-mgq\\)\nsolving the phase equation we have\n\\(\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q}\\)\n\\(F=-mg\\)\nwhich is true, (\\(F=\\frac{dp}{dt}\\))\nand solving the position equation we have\n\\(\\frac{dq}{dt}=\\frac{p}{m}\\)\nwhich is funnily a redundant equation saying\n\\(v=\\frac{mv}{m}\\)\nThe more apt description of the system in Hamiltonian mechanics is though\n\\(\\dot p=-mg\\)\n\\(\\dot q=\\frac{p}{m}\\)\nwhich are used as coupled first-order ODEs which can be mathematically easier to solve.\n\n\nMomentum from the Lagrangian\n\\(p=\\frac{\\partial L}{\\partial \\dot q}\\)\nWas defined by Lagrange beforehand- not as a relation to Hamiltonian Mechanics.\nThe legendre transform is mathematically\n\\(Df=(Df^*)^{-1}\\)\nbut is defined mechanically as\n\\((q,\\dot q) \\rightarrow (p,q)\\)\nand it can be used to turn the Lagrangian into the Hamiltonian.\n\\(\\frac{d}{dt}(\\frac{\\partial{L}}{\\partial{\\ddot{x}}})=-\\frac{\\partial{L}}{\\partial{x}}\\)\nso\n\n\nAside: Definition of Hamiltonian from Lagrangian\n\\(H=\\sum_i^n{p_i\\dot q^i}-L\\)\nso then if \\(p_i\\dot q^i\\) is \\(mv^2/2\\) then simply\n\\(H=T-(T-V)=V\\)???\n\n\nDerivation of the Lagrange-Euler Equations\nThis is called the “principle of least action”.\nThe action \\(S\\) must be minimized\n\\(S\\equiv \\int_{t_0}^{t_1}Ldt\\)\nso then this is equivalent to saying that any small perturbation on \\(L\\) must not affect the value of \\(S\\)\nso then some \\(x(t)\\) solution to this that is perturbed a little bit by some arbitrary function that is zero at the bounds \\(t_0\\) and \\(t_1\\)\n\\(x(t)=x_0(t)+a\\beta(t)\\)\nshould not change \\(S\\) for small changes in \\(a\\) so then\n\\(\\frac{dS}{da}=0\\)\n\\(\\frac{dS}{da}=\\int_{t_0}^{t_1}\\frac{dL}{da}dt\\)\n\\(\\frac{dS}{da}=\\int_{t_0}^{t_1}(\\frac{dL}{dx}\\frac{dx}{da}+\\frac{dL}{d\\dot x}\\frac{d\\dot x}{da})dt\\)\nbut note that\n\\(\\frac{dx}{da}=\\beta(t)\\) and\n\\(\\dot x=\\frac{dx}{dt}=\\frac{dx_0}{dt}+a\\frac{d\\beta}{dt}\\)\nso\n\\(\\frac{d\\dot x}{dt}=\\dot \\beta\\)\nand then\n\\(\\frac{dS}{da}=\\int_{t_0}^{t_1}(\\frac{dL}{dx}\\beta+\\frac{dL}{d\\dot x}\\dot \\beta )dt\\)\nand then we can integrate by parts the latter side since \\(\\dot \\beta dt=d\\beta\\)\n\\(\\int \\frac{dL}{d \\dot x}d\\beta=\\frac{dL}{d\\dot x}\\beta-\\int\\beta du\\)\nand \\(\\frac{du}{dt}=\\frac{d}{dt}\\frac{dL}{d\\dot x}\\)\n\\(\\int \\frac{dL}{d \\dot x}d\\beta=\\frac{dL}{d\\dot x}\\beta-\\int\\beta \\frac{d}{dt}\\frac{dL}{d\\dot x}dt\\)\nso then\n\\(\\frac{dS}{da}=\\int_{t_0}^{t_1}(\\frac{dL}{dx}\\beta+\\beta \\frac{d}{dt}\\frac{dL}{d\\dot x})dt-\\frac{dL}{d\\dot x}\\beta \\bigg\\rvert_{t_0}^{t_1}\\)\nbut \\(\\beta(t_1)=\\beta(t_0)=0\\) so then \\(-\\frac{dL}{d\\dot x}\\beta \\bigg\\rvert_{t_0}^{t_1}=0\\)\nand remains\n\\(\\frac{dS}{da}=\\int_{t_0}^{t_1}(\\frac{dL}{dx}\\beta+\\beta \\frac{d}{dt}\\frac{dL}{d\\dot x})dt\\)\n\\(\\frac{dS}{da}=\\int_{t_0}^{t_1}\\beta\n(\\frac{dL}{dx}+ \\frac{d}{dt}\\frac{dL}{d\\dot x})dt\\)\nnow we’re arguing that for any function \\(\\beta(t)\\)** this evaluates to zero.**\nConsider the case where \\(\\frac{dL}{dx}+ \\frac{d}{dt}\\frac{dL}{d\\dot x}\\neq0\\). Then, we can choose also a nonzero \\(\\beta(t)\\) for that portion and see that \\(\\frac{dS}{da}\\neq0\\) for smooth, integrable \\(L\\) on \\(x,\\dot x\\). Therefore,\n\\(\\frac{dL}{dx}+ \\frac{d}{dt}\\frac{dL}{d\\dot x}=0\\)\nwhich is the Euler-Lagrange equation.\n\n\nEvaluating for polynomial with zeros only at the bounds\nLet’s say we have a falling particle in a gravity field. Then,\n\\(L=T-V=\\frac{1}{2}my'^2-mgy\\)\n\\(y''=-g\\)\n\\(y=-\\frac{1}{2}gt^2\\)\nLet’s pretend the bounds are \\(t\\in[t_0,t_1]\\). Then,\n\\(y=-\\frac{1}{2}gt^2+\\epsilon (t-t_0)(t-t_1)\\)\nshould make it so that \\(\\frac{dS}{d\\epsilon}\\rightarrow0\\) as \\(\\epsilon \\rightarrow 0\\) i.e. \\(\\frac{dS}{d\\epsilon}\\bigg\\rvert_{\\epsilon=0}=0\\)\n\\(\\frac{dS}{d\\epsilon}=\\int_{t_0}^{t_1}{\\frac{dL}{d\\epsilon}dt}\\)\n\\(\\int(my'\\frac{dy'}{d\\epsilon}-mg\\frac{dy}{d\\epsilon})dt\\)\n\\(y'=-gt+\\epsilon(2t-t_0-t_1)\\)\n\\(\\frac{dy'}{d\\epsilon}=2t-t_0-t_1\\)\n\\(\\frac{dy}{d\\epsilon}=(t-t_0)(t-t_1)\\)\n\\(m\\int((-gt+\\epsilon(2t-t_0-t_1))(2t-t_0-t_1)-g(t-t_0)(t-t_1))dt\\)\n\\(m\\int(-gt(2t-t_0-t_1)+\\epsilon(2t-t_0-t_1^2)-g(t-t_0)(t-t_1))dt\\)\nThen, because epsilon goes to zero, we can remove epsilon terms.\n\\(m\\int(-gt(2t-t_0-t_1)-g(t^2-t(t_0+t_1)+t_0t_1))dt\\)\n\\(mg\\int(-2t^2+t(t_0+t_1)-t^2+t(t_0+t_1)-t_0t_1))dt\\)\n\\(mg\\int(-3t^2+2t(t_0+t_1)-t_0t_1))dt\\)\n\\(mg[-t^3+t^2(t_0+t_1)-tt_0t_1]\\big\\rvert_{t_0}^{t_1}\\)\n\\(mg([-t_1^3+t_1^3+t_1^2t_0-t_1^2t_0]-[-t_0^3+t_0^3+t_1t_0^2-t_0^2t_1])\\)\n\\(=0\\)\n\n\nWhat about for completely arbitrary perturbation functions?\n\\(f(t_0)=0,f(t_1)=0\\)\nLet’s pretend the bounds are \\(t\\in[t_0,t_1]\\). Then,\n\\(y=-\\frac{1}{2}gt^2+\\epsilon f(t)\\)\nshould make it so that \\(\\frac{dS}{d\\epsilon}\\rightarrow0\\) as \\(\\epsilon \\rightarrow 0\\) i.e. \\(\\frac{dS}{d\\epsilon}\\bigg\\rvert_{\\epsilon=0}=0\\)\n\\(y'=-gt+\\epsilon f'(t)\\)\n\\(\\frac{dy'}{d\\epsilon}=f'(t)\\)\n\\(\\frac{dy}{d\\epsilon}=f(t)\\)\nThen if\n\\(L=\\frac{my'^2}{2}-mgy\\)\n\\(\\int(my'\\frac{dy'}{d\\epsilon}-mg\\frac{dy}{d\\epsilon})dt\\)\n\\(\\int(m(-gt+\\epsilon f'(t))f'(t)-mgf'(t))dt\\)\n\\(m\\int(-gtf'(t)+\\epsilon f'(t)^2-gf'(t))dt\\)\n\\(m\\int(-(gt+g)f'(t)+\\epsilon f'(t)^2)dt\\)\nThen, again, \\(\\epsilon \\rightarrow 0\\)\n\\(m\\int(-(gt+g)f'(t))dt\\)\n\\(mg\\int(-(t+1)f'(t))dt\\)\n\\(mg\\int-(t+1)f'(t)dt\\)\n\\(mg\\int-(t+1)f'(t)dt\\)\n\\(mg(\\int-tf'(t)dt-\\int f'(t)dt)\\)\nRemove mg, since that doesn’t matter, then distribute\n\\(\\int-tf'(t)dt-f(t)\\rvert_{t_0}^{t_1}\\)\n\\(\\int-tf'(t)dt\\)\nthen via substitution\n\\(du=f'(t)dt\\)\n\\(u=f(t)\\)\n\\(\\int-tdu\\)\n\\(-ut\\)\n\\(-tf(t)\\big\\rvert_{t_0}^{t_1}\\)\n\\(0\\)\nTurns out, the subsitution above is illegal because \\(t=f^{-1}(u)\\)\nMaking the solution more general than this would require some sort of general description of functions that solve the EL equation? Or actually, can any lagrangian work so long as \\(T\\) describes a kinetic-like energy and \\(V\\) describes a conservative field?\n\\(\\frac{d}{dt}(\\frac{dL}{dy'})=-\\frac{dL}{dy}\\)\n\n\nGeneralized d-dimensional Dirac Delta\n\\(\\int d^dx\\delta^{(d)}(x)=1\\)\nthen the fourier transform is\n\\(\\int e^{ikx}d^dx\\delta^{(d)}(x)\\)\nbut via the sifting property\n\\(\\int f(x) \\delta^{(d)}(x)d^{d}x=f(0)\\)\nso then\n\\(\\int e^{ikx}d^dx\\delta^{(d)}(x)=e^{ik0}=1\\)"
  },
  {
    "objectID": "notes/Methods in Quantization.html",
    "href": "notes/Methods in Quantization.html",
    "title": "Methods in Quantization for TinyML Frameworks",
    "section": "",
    "text": "TODO: Add commentary, since we’re publicizing this one now. \n\n\nQuantized Averagepool\n\\(\\sum_i^n r_i=\\sum_i^n s(q_i-z)\\)\n\\(=\\frac{s[(\\sum_i^nq_i)-nz]}{n}\\)\n\\(=\\frac{s}{n}\\sum q_i-sz\\)\n\nAccounting for the output scale…\n\\(r_y=\\sum_i^n r_{xi}=\\sum_i^n s_x(q_{xi}-z_x)\\)\n\\(r_y=\\frac{s_x[(\\sum_i^nq_{xi})-nz_x]}{n}\\)\n\\(r_y=\\frac{s_x}{n}\\sum q_{xi}-sz_x\\)\n\\(s_y(q_y-z_y)=\\frac{s_x}{n}\\sum q_{xi}-sz_x\\)\n\\(q_y=\\frac{s_x}{s_yn}\\sum q_{xi}-\\frac{s_x}{s_y}z_x+z_y\\)\n\n\n\nTFlite Quantized Matmul\nTensorflow Lite inference - how do I scale down the convolution layer outputs?\n\nthe output of a multiplication int8 x int8 is also scaled by some multiplier M.\n\n\\(Y=AX\\)\n\\(M=\\frac{S_AS_X}{S_Y}\\) (see google paper on tflite https://arxiv.org/pdf/1712.05877)\n\\(M\\in[0,1]\\) empirically.\n\nwe represent this with a bitshift and a number \\(M_0\\in[0.5,1]\\)\n\n\\(M=2^{-n}M_0\\)\n\n\\(M_0\\) is representable in fixed-point with purely fractional bits.\nIf the initial multiplication result is a 16-bit \\(R\\) and \\(M_0\\) is also 16-bit, then \\(RM_0\\) is a 32-bit number.\nWe then &gt;&gt; shift out 16 bits (as many as the bits of \\(M_0\\)) of \\(RM_0\\) (as many as the result, then apply \\(2^{-n}\\) to it in shifts.\nFinally, apply a saturating clip to 8 bits to the result. (Take the lowest outBits bits)\n\ndef convert_scale_to_shift_and_m0(scale,precision=16):\n    \" Convert scale(s) to shift and zero point \"\n\n    shift = int(np.ceil(np.log2(scale)))\n    # shift = np.abs(shift)\n    m0 = scale / 2**shift\n    fp_string = convert_to_fixed_point(m0,precision)\n    m0_clipped = fixed_point_to_float(fp_string,precision)\n    return m0_clipped, shift\n\nvconvert_scale_to_shift_and_m0 = np.vectorize(convert_scale_to_shift_and_m0)\n\ndef convert_to_fixed_point(number,precision):\n    \" Convert a float [0,1] to fixed point binary string\"\n    out = ''\n    for i in range(precision):\n        number *= 2\n        integer = int(number)\n        number -= integer\n        out += str(integer)\n    return out\n\ndef convert_to_fixed_point_int(number,precision):\n    \" Convert a float [0,1] to fixed point binary \"\n    return int(convert_to_fixed_point(number,precision),base=2)\n\ndef fixed_point_to_float(number,precision):\n    \" Convert a fixed point binary to float [0,1] \"\n    out = 0\n    for i in range(precision):\n        out += int(number[i]) * 2**-(i+1)\n    return out\nm0, shift = q.convert_scale_to_shift_and_m0(0.019110053777694702)\nfp_int = q.convert_to_fixed_point_int(m0,16)\n(m0 * 2**(shift)) - (fp_int * 2**(shift-16))\n\nZeroes Thereof\n\n\n\n\nFull Integer Quantized Addition\nopenaccess.thecvf.com — Supplementary information of the TFLite paper\n\nSome neural networks use a plain Addition layer type, that simply adds two activation arrays together. Such Addition layers are more expensive in quantized inference compared to floating-point because rescaling is needed: one input needs to be rescaled onto the other’s scale using a fixedpoint multiplication by the multiplier \\(M = S_1/S_2\\) similar to what we have seen earlier before actual addition can be performed as a simple integer addition; finally, the result must be rescaled again to fit the output array’s scale8.\n\nTheir implementation in the tflite code: \nhttps://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h#L1402-L1507\n  for (; i &lt; size; i++) {\n    const int32 input1_val = input1_offset + input1_data[i];\n    const int32 input2_val = input2_offset + input2_data[i];\n    const int32 shifted_input1_val = input1_val * (1 &lt;&lt; left_shift);\n    const int32 shifted_input2_val = input2_val * (1 &lt;&lt; left_shift);\n    const int32 scaled_input1_val = MultiplyByQuantizedMultiplierSmallerThanOne(\n        shifted_input1_val, input1_multiplier, input1_shift);\n    const int32 scaled_input2_val = MultiplyByQuantizedMultiplierSmallerThanOne(\n        shifted_input2_val, input2_multiplier, input2_shift);\n    const int32 raw_sum = scaled_input1_val + scaled_input2_val;\n    const int32 raw_output = MultiplyByQuantizedMultiplierSmallerThanOne(\n                                 raw_sum, output_multiplier, output_shift) +\n                             output_offset;\n    const int32 clamped_output = std::min(\n        output_activation_max, std::max(output_activation_min, raw_output));\n    output_data[i] = static_cast&lt;uint8&gt;(clamped_output);\n  }\n\n\nErrors based on the TFLite output scaling\n\nGraph\n\nx — internal precision\ny — order of error\n(16, ^-10)\n\n\n\n\nGoogle Paradigm on Dealing with Low Precision Matrix Multiplication\nAlso see:\n— Basically also discussed in the TFLite paper\nhttps://github.com/google/gemmlowp/blob/master/doc/quantization.md\n\n\nLiteRT Torch Converter\n\nCan convert with quantization using either torch’s PT2E or TFLite.\n\nSee https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/pytorch_converter/README.md\n\n\n\n\n\nOn EdMIPS\nhttps://www.figma.com/file/9pmqozMDAtTJckTQEhCzhj/300B?type=whiteboard&node-id=21-135&t=8svuD9WRbEzFTHpY-4\n\n\nBiases\n\\(\\hat{I}\\cdot\\hat{W}+\\hat{B}\\neq I\\cdot W+B\\)\n\n\nStatic vs Dynamic Quantization\nThe value distribution of the input feature maps can vary from input to input, making it difficult to specify optimal quantization ranges.\n\nDynamic Quantization — Specify quantization range for each activation fmap before calculation.\n\n\nStatic Quantization — Specify range beforehand\n\nUse a series of calibration inputs to compute the typical range using:\n\nMinimize MSE\nKL Divergence (entropy)\nImpose clipping range during training\n\n\n\n\nWorks\n\nLQNets\nPACT\nLSQ/LSQ+\n\n\n\n\nStochastic Quantization\n\nTypically only for online learning\n\nFor online learning with quantization, small weight updates may not necessarily lead to large changes in weight. To effectively still reflect the small changes, we can keep a probability \\(x\\) separate from the rounded \\(\\lfloor x \\rceil\\) integer x. In this case,\n\\(\\hat{x} = \\begin{cases}\n\\lfloor x \\rfloor \\text{ with probability } \\lceil x \\rceil - x  \\\\\n\\lceil x \\rceil \\text{ with probability } x - \\lfloor x \\rfloor\\\\\n\\end{cases}\\)\n\n\nSimulated & Integer-only Quantization\n\nIn simulated quantization, the quantized model parameters are stored in low-precision, but the operations (e.g. matrix multiplications and convolutions) are carried out with floating point arithmetic. - Gholami ’22 LPCV\n\n\nSimulated Quantization is “fake” quantization needing dequantization before every operation. EdMIPS uses simulated quantization in order to perform their differential architecture search with finetuning while searching for the optimal weight quantization set \\(\\{o_i,o_w\\}\\).\n\n\n\n\n\n\n+ Fact: The HWGQ Quantizer from EdMIPS is simulated quantization. See the *step portion of the formula.\n\n\n\n class _hwgq(torch.autograd.Function):\n \n     @staticmethod\n     def forward(ctx, x, step):\n         y = torch.round(x / step) * step\n         return y\n \n     @staticmethod\n     def backward(ctx, grad_output):\n         return grad_output, None\n\n\nIn contrast, works like MCUNet and AIMC use integer-only quantization specifically for the reason of energy efficiency.\n\nSome hardware processors, including NVIDIA V100 and Titan RTX, support fast processing of low-precision arithmetic that can boost the inference throughput and latency. — Gholami ’22\n\nPeng ’21 Neurosim in a work on fully-integer based quantization for mobile CNN inference mention in their introduction:\n\n… the dot product of two bit vectors x and y can be computed using the following formula [24], where \\(x_i,y_i\\in\\{0,1\\}\\forall i\\) and the ’’bitcount” operation counts the number of bits that have a value of one in each vector element\n\n\\(x\\cdot y=bitcount(x AND y)\\)\nThis is exactly how we perform 1b W x 1b I in SRAM-IMC.\n\n\nDetermining step/range vs bits\n\ndef quantize_model_weights(model, bits, step):\n\n        # qm = EdMIPS quantization module\n\n    quantized_state_dict = model.state_dict()\n    for name, param in model.named_parameters():\n        quantized_state_dict[name] = qm._gauss_quantize.apply(\n            param,\n            step,\n            bits\n        )\n    model.load_state_dict(quantized_state_dict)\n\n    return model\n\n\n\n\nThe first layer of DS-CNN quantized with a 4-bit Gaussian Quantizer with step=0.336. You can see that 16 levels are available.\n\n\n\n\nEdMIPS Quantizer Range\nFrom these two and the corresponding code, we now have a working quantizer function.\nWe now need to look into the literature in order to find the optimal quantizer range. IIRC, this is mentioned in:\n\nHWGQ (though this is for 1-bit)\nEdMIPS \\((\\sigma t_i,\\sigma t_{i+1})\\)\n\n\nA quantizer is the set of ranges for which you clip values into specific integers.\n\nThe optimal quantizer can be found by way of Lloyd’s Algorithm (k-means clustering). However, this is difficult to perform for since each weight group has a different distribution. This is simplified by two ideas:\n\nWeight distributions are typically gaussian. Hence, we only need the mean and variance in order to specify the quantizer.\nBy using batchnorm, the weight means and variances are all 0 and 1, allowing us to use the same quantizer for all layers. This means that we only have to solve the Lloyd’s algorithm once. Lloyd’s algorithm restrained for uniform quantization for standard gaussian \\(\\sigma=1,\\mu=0\\) gives results as follows:\n\ngaussian_steps = {1: 1.596, 2: 0.996, 3: 0.586, 4: 0.336}\nhwgq_steps = {1: 0.799, 2: 0.538, 3: 0.3217, 4: 0.185}\n\nEdMIPS further assumes that the means are all 0 but the variances are non-1. This means that measuring the standard deviation per parameter set (weight layer) is enough. The step for the quantization can then be scaled as \\(\\sigma*step\\). Hence, the gauss quantize function becomes as follows.\n\nclass _gauss_quantize(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, step, bit):\n        lvls = 2 ** bit / 2\n        alpha = x.std().item()\n        step *= alpha\n        y = (torch.round(x/step+0.5)-0.5) * step\n        thr = (lvls-0.5)*step\n        y = y.clamp(min=-thr, max=thr)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output, None, None\n\nWe can observe there’s a constant -step*0.5 portion to the y, which shows itself as the lack of a “0” in our quantized weight histograms.\n\n\n\n\nJacob ’18 CVPR “Quantization & Training of NN for Efficient INT-only Arithmetic” synced block\n\n\n\n\n\nThe first layer of DS-CNN quantized with a 4-bit Gaussian Quantizer with step=0.336. You can see that 16 levels are available.\nJacob ’18 CVPR “Quantization & Training of NN for Efficient INT-only Arithmetic” synced block"
  },
  {
    "objectID": "notes/RingsAndFields.html",
    "href": "notes/RingsAndFields.html",
    "title": "Rings and Fields",
    "section": "",
    "text": "Abstract algebra is apparently the study of rings and fields plus groups…"
  },
  {
    "objectID": "notes/RingsAndFields.html#why-rings",
    "href": "notes/RingsAndFields.html#why-rings",
    "title": "Rings and Fields",
    "section": "Why rings?",
    "text": "Why rings?\nCommutative algebra is the study of rings. Rings were first delineated when the concept of ideals came about.\nRings have important common properties we take advantage of like unique factorization domains (a type of ring), and a bunch of other mouthful theorems like\n\nMaximal Ideals are one-to-one with Fields (see later section) (\\(M \\text{ maximal} \\iff R/M \\text{ is a field}\\))\n\\(R/\\text{ker}(\\phi)\\cong \\text{im}(\\phi)\\).\n\\(u \\in R\\) is a unit iff \\(\\text{Ideal}(u)=R\\) (defines what a “unit” element is, like 1 or i. Probably useful for induction on weird sets too?)\n\nTl;DR, things close to rings (groups, commutative rings, rings with unique factorization, fields) are useful. Theorems that apply ONLY to rings themselves are a little abstract (heh, pun intended).\nIdeals are subsets of rings (technically called a sublattice, probably because it’s a “discrete vector field” because it’s a linear combination of the ring elements)\nIdeals can be factored, and can be prime.\n\n\n\n\n\n\nWhat is an Ideal?\n\n\n\nAn ideal \\(I\\) is a subset of the ring that absorbs multiplication. If \\(r \\in R\\) and \\(x \\in I\\), then \\(rx \\in I\\). Notation: \\((a, b)\\) is the ideal generated by \\(a\\) and \\(b\\). It implies the set of all linear combinations \\(\\{ra + sb \\mid r, s \\in R\\}\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Real ring \\(\\mathbb{R}\\) has exactly two ideals:\n\nZero ideal (0)\nEntire ring (1)\n\nIt is trivial. This is because when trying to define any other ideal, say (0.001), you accidentally become the (1) ideal because you always have a multiplative inverse.\n\n\n\n\n\n\n\n\nMaximal vs Prime Ideals\n\n\n\nMaximal Ideal: An ideal \\(M\\) is maximal if no other proper ideal \\(J\\) contains \\(M\\) (i.e., \\(M\\subset J\\subset R\\) is impossible). Prime Ideal: An ideal \\(P\\) is prime if for any \\(a,b\\) in the ring, if \\(ab\\in P\\), then \\(a\\in P\\) or \\(b\\in P\\)"
  },
  {
    "objectID": "notes/RingsAndFields.html#function-ideals-and-cotangent-space",
    "href": "notes/RingsAndFields.html#function-ideals-and-cotangent-space",
    "title": "Rings and Fields",
    "section": "Function Ideals and Cotangent space",
    "text": "Function Ideals and Cotangent space\n\n\n\n\n\n\nEssence\n\n\n\nLinking Ideals and Geometry forms the power of abstract algebra\n\n\n\nTODO: Fill in later.\n\n\nQuotient Notation\n\\[R = \\frac{\\mathbb{R}[x,y]}{(y^2 - x^3)}\\] \\[\\text{Cotangent Space} = \\frac{\\mathfrak{m}}{\\mathfrak{m}^2}\\]"
  },
  {
    "objectID": "notes/Solving Ballistics Equation with RK4 in C.html",
    "href": "notes/Solving Ballistics Equation with RK4 in C.html",
    "title": "Solving Ballistics Equation with RK4 in C",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do we encode the ballistics equation in C?\n\n\n\nSo the most basal ballistics equation is \\[\\mathbf{F}=m\\vec{a}\\]\nThen the vectors are \\(\\mathbf{F},x,y,\\frac{dx}{dt},\\frac{dy}{dt}\\). The state vector \\[\\vec{S}=\\begin{bmatrix}\nx\\\\y\\\\dx/dt\\\\dy/dt\n\\end{bmatrix}\\]\n\nForward Euler\n \\[\\vec{S}_{n+1}=\\mathbf{U}\\vec{S_n}+\\vec{F}\\] and since \\(x_{n+1}=\\Delta t \\frac{dx}{dt}+x_n,y_{n+1}=\\Delta t \\frac{dy}{dt}+y_n,\\)\nThe update matrix is \\[\\mathbf{U}=\\begin{bmatrix}\n1 & 0 & dt & 0 \\\\\n0 & 1 & 0 & dt \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}, \\vec{F}=\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ -g\\cdot dt\n\\end{bmatrix}\\]\nSo first let’s try to make some vector-matrix working and test it out\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// Not sure if it's a good idea to name these such a generic name\n// How do we handle other datatypes of vectors?\ntypedef struct vector {\n    size_t length;\n    float* elements; \n} vector_t;\n\ntypedef struct matrix {\n    size_t rows;\n    size_t cols;\n    float* elements;\n} matrix_t;\n\n// Any naming scheme for this to keep it local? \n// There aren't any classes in C, but I've heard \n// significant criticism of C++ anyway.\nfloat matrix_get_element (matrix_t* mat, int i, int j) {\n\n    if (i &gt; mat-&gt;rows || j &gt; mat-&gt;cols) {\n        // Don't know standard practice in raising error.\n        fprintf(stderr, \"[matrix_get_element] (%i,%i) out of range. Exiting.\",i,j);\n        exit(EXIT_FAILURE);\n    };\n    \n    return mat-&gt;elements[mat-&gt;cols*i + j];\n}\n\nvoid vector_print (vector_t* vec, char* name) {\n    int i;\n\n    printf(\"%s &lt;%zu&gt; : [\\n\",name,vec-&gt;length);\n    for (i=0;i&lt;vec-&gt;length;i++) {\n        printf(\"  %f,\\n\",vec-&gt;elements[i]);\n    };\n    printf(\"]\\n\");\n}\n\nvoid matrix_print (matrix_t* mat, char* name) {\n    printf(\"%s &lt;%zu,%zu&gt;: [\\n\",name,mat-&gt;rows,mat-&gt;cols);\n    for (int i = 0; i &lt; mat-&gt;rows; i++) {\n        for (int j = 0; j &lt; mat-&gt;cols; j++) {\n            printf(\"  %f,\\t\",matrix_get_element(mat,i,j));\n        }\n        printf(\"\\n\");\n    }\n    printf(\"]\\n\");\n\n}\n\nvoid matrix_vector_multiply (vector_t* res, matrix_t *mat, vector_t* vec) {\n    if (mat-&gt;rows != vec-&gt;length) {\n        fprintf(stderr,\"mat rows != vector len! (%zu!=%zu)\", mat-&gt;rows, vec-&gt;length);\n        exit(EXIT_FAILURE);\n    };\n\n    int i,j;\n    for (i = 0;i &lt; res-&gt;length;i++) {\n        res-&gt;elements[i] = 0.;\n        for (j = 0;j &lt; vec-&gt;length;j++) {\n            res-&gt;elements[i] += vec-&gt;elements[j] * matrix_get_element(mat, i, j);\n            //printf(\"[%f,%f]\\n\",vec-&gt;elements[j],matrix_get_element(mat, i, j));\n        };\n    };\n};\n\nvoid vector_add (vector_t* res, vector_t* veca, vector_t* vecb) {\n\n    if (veca-&gt;length != vecb-&gt;length) {\n        fprintf(stderr,\"vector length mismatch (%zu!=%zu)\",veca-&gt;length,vecb-&gt;length);\n        exit(EXIT_FAILURE);\n    }\n\n    for (int i=0; i&lt;veca-&gt;length; i++) {\n        res-&gt;elements[i] = veca-&gt;elements[i] + vecb-&gt;elements[i];\n    }\n\n}\n\nvoid sim_step (vector_t* next_state, matrix_t* update, vector_t* state, vector_t* force) {\n    matrix_vector_multiply(next_state,update,state);\n    vector_add(state,next_state,force);\n    //vector_print(state,\"State\");\n}\n\nvoid matrix_place_vector (matrix_t* mat, vector_t* vec, int index) {\n    \n    if (mat-&gt;cols != vec-&gt;length) {\n        fprintf(stderr,\"mat cols != vector len! (%zu!=%zu)\", mat-&gt;rows, vec-&gt;length);\n        exit(EXIT_FAILURE);\n    };\n\n    for (int j=0; j &lt; vec-&gt;length; j++) {\n        mat-&gt;elements[index*mat-&gt;cols + j] = vec-&gt;elements[j];\n    }\n    \n}\n\nint main(void) {\n    matrix_t update_matrix;\n    vector_t force_vector;\n    float dt=1e-3;\n    float g=9.81;\n    \n    //float stop_time = 10; //seconds\n    int num_steps = 500; \n    printf(\"Number of steps: %i\\n\",num_steps);\n    if (num_steps &gt; 1000) {\n        printf(\"Warning: simulating more than 1000 steps.\\n\");\n    }\n\n    matrix_t result_matrix;\n    float result_matrix_elements[num_steps*4];\n    result_matrix.cols = 4;\n    result_matrix.rows = num_steps;\n    result_matrix.elements = result_matrix_elements;\n        \n    vector_t state;\n    float state_elements[4] = {\n        0.,\n        0.,\n        1.,\n        1.\n    };\n    state.length = 4;\n    state.elements = state_elements;\n\n    float update_matrix_elements[4*4] = {\n        1,  0,  dt,  0 ,\n        0,  1,  0 ,  dt,\n        0,  0,  1 ,  0 ,\n        0,  0,  0 ,  1 ,\n    };\n    update_matrix.rows = 4;\n    update_matrix.cols = 4;\n    update_matrix.elements = update_matrix_elements;\n\n    float force_vector_elements[4] = {\n        0,\n        0,\n        0,\n        -g*dt\n    };\n    force_vector.length = 4;\n    force_vector.elements = force_vector_elements;\n\n    matrix_print(&update_matrix, \"Update Matrix\");\n    vector_print(&force_vector, \"Force Vector\");\n    vector_print(&state,\"Initial State\");\n\n    vector_t next_state;\n    float next_state_elements[4];\n    next_state.elements = next_state_elements;\n    next_state.length = 4;\n\n    for (int step=0;step&lt;num_steps;step++) {\n        sim_step(&next_state,&update_matrix,&state,&force_vector);\n        matrix_place_vector(&result_matrix,&state,step);\n    }\n\n    matrix_print(&result_matrix,\"Results\");\n\n};\n\n\n\n\n\n\nResults\n\n\n\nForward Euler always seems to overshoot the analytical solution. This is likely because we reduce the velocity over time, but we calculate the change in position with the previous velocity.\nSo then, \\(x_{n+1}=x_n+\\Delta t v_n\\).\n\n\n\n\nPlotting\nPlan: let’s just pass the matrix to matplotlib. An interactive plotting framework is nice to implement, but that’s not what we’re doing right now.\nI implemented a pass between C and Python\n// Nearly a general array\nint tensor_write_bin (const char* filename, const Matrix* mat) {\n\n    FILE* file = fopen(filename,\"wb\");\n    if (!file) {\n        fprintf(stderr, \"Failed to open file.\");\n        return 1;\n    }\n    \n    // Start file with ndims\n    size_t ndims = 1;\n    fwrite(&ndims, sizeof(size_t), 1, file);\n\n    // File starts with rows and cols\n    fwrite(&mat-&gt;rows, sizeof(size_t), 1, file);\n    fwrite(&mat-&gt;cols, sizeof(size_t), 1, file);\n\n    // Then, the payload\n    size_t matBufferLength = mat-&gt;rows*mat-&gt;cols;\n    fwrite(mat-&gt;elements, sizeof(float), matBufferLength, file);\n\n    return 0;\n}\ndef read_array_bin(filename):\n\n    size_t = ctypes.sizeof(ctypes.c_size_t);\n\n    if size_t == 4:\n        datatype = np.uint32;\n    else:\n        datatype = np.uint64;\n\n    with open(filename,'rb') as f:\n        ndims = np.fromfile(f,datatype,1)[0];\n        shape = np.fromfile(f,datatype,ndims); \n        array = np.fromfile(f,np.float32,1000);\n    \n    return array.reshape(shape)\nThen we use the matplotlib animations library to animate it:\n    res_array = read_array_bin(args.input_file)\n    print(res_array)\n\n    x_pos = res_array[0]\n    y_pos = res_array[1]\n\n    fig, ax = plt.subplots()\n    line = ax.plot(x_pos[0], y_pos[0], label=f'Trajectory')[0]\n    \n    ax.set(xlim=[0,20],ylim=[-5.5,5.5],xlabel='x', ylabel='y')\n    ax.legend()\n\n    def update(frame):\n        x = x_pos[:frame]\n        y = y_pos[:frame]\n        line.set_xdata(x)\n        line.set_ydata(y)\n        return (line,line)\n\n    dt = 5e-3\n    frames = res_array.shape[1] # Total number of frames\n    interval = dt # Delay between frames\n\n    ani = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=dt)\n    plt.show()\n\n    fps = int(1/dt)\n    ani.save(f'{filename}.gif',fps=60)\n\n\n\nTrajectory without drag\n\n\n\n\nForward Euler with Quadratic Drag\nThe quadratic drag equation has (mostly) no analytical solutions.\nWith drag, a force proportional to the square of the velocity develops opposing the direction of the velocity: \\[\\vec{F}=-k|\\vec{v}|\\vec{v}\\] The update matrix turns into: \\[\\mathbf{U}=\\begin{bmatrix}\n1 & 0 & dt & 0 \\\\\n0 & 1 & 0 & dt \\\\\n0 & 0 & 1-k|v| & 0 \\\\\n0 & 0 & 0 & 1-k|v| \\\\\n\\end{bmatrix}, \\vec{F}=\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ -g\\cdot dt\n\\end{bmatrix}\\]\nTo implement drag, we also update the update matrix every time step:\nvoid get_update_matrix (Matrix* upd,const Vector* state,const float dt, const float drag_constant) {\n\n    float x = state-&gt;elements[0];\n    float y = state-&gt;elements[1];\n    float vx = state-&gt;elements[2];\n    float vy = state-&gt;elements[3];\n\n    float speed = sqrtf(vx*vx + vy*vy); \n    float drag = speed*drag_constant;\n\n    float updated_elements[4*4] = {\n        1,  0,  dt     ,  0      ,\n        0,  1,  0      ,  dt     ,\n        0,  0,  1-drag ,  0      ,\n        0,  0,  0      ,  1-drag ,\n    };\n\n    memcpy(upd-&gt;elements, updated_elements, sizeof(updated_elements));\n\n}\n\n\n\nForward Euler with Drag\n\n\nNotice that with quadratic drag, the system is now nonlinear and the “update matrix” no longer makes sense (it’s a function of the state vector it’s operating on, so it’s not linear). We can replace the matrix stuff with a generic update of the state:\nvoid sim_step (Vector* state,const Vector* force, const float dt, const float drag_per_mass) {\n\n    float x = state-&gt;elements[0];\n    float y = state-&gt;elements[1];\n    float vx = state-&gt;elements[2];\n    float vy = state-&gt;elements[3];\n\n    float speed = sqrtf(vx*vx + vy*vy);\n\n    float dx = vx;\n    float dy = vy;\n    float dvx = vx*(-speed*drag_per_mass) + force-&gt;elements[2];\n    float dvy = vy*(-speed*drag_per_mass) + force-&gt;elements[3];\n    \n    state-&gt;elements[0] = x + dt*dx;\n    state-&gt;elements[1] = y + dt*dy;\n    state-&gt;elements[2] = vx + dt*dvx;\n    state-&gt;elements[3] = vy + dt*dvy;\n\n}\n\n\nRunge-Kutta 4\nForward euler is technically RK1. The 4 means we approximate the correct average slope over the time step with 4 different estimates.\n\n\n\n\n\n\nRunge-Kutta\n\n\n\nGiven a problem in the following form: \\[\\frac{dy}{dt}=f(t,y), y(t_0)=y_0\\] RK4 gives an estimate of \\(\\frac{dy}{dt}\\) using a weighted average of four slopes \\(f(t,y)\\) \\(k_1\\) to \\(k_4\\) in the “future” of the current point in time Choosing a time step \\(\\Delta t&gt;0\\) \\[y_{n+1}=y_n+\\frac{\\Delta t}{6}(k_1+2k_2+2k_3+k_4)\\] \\[t_{n+1}=t_n+\\Delta t\\] \\[k_1=f(t_n,y_n),\\] \\[k_2=f(t_n+\\frac{\\Delta t}{2},y_n+\\Delta t\\frac{k_1}{2})\\] \\[k_3=f(t_n+\\frac{\\Delta t}{2},y_n+\\Delta t\\frac{k_2}{2})\\] \\[k_4=f(t_n+\\Delta t,y_n+\\Delta tk_3)\\]\n\n\nIf the forward euler equation for \\(v_y\\) is \\[v_{y,t+1}=f(t_n,v_y)=v_{y,t} -g\\Delta t-k|\\vec{v}|v_{y,t}\\] Then the RK4 estimates would be \\[k_1=v_{y,t} (1-k|\\vec{v}|) -g\\Delta t\\] \\[k_2=(v_{y,t}+\\frac{\\Delta tk_1}{2}) (1-k|\\vec{v}(\\frac{\\Delta tk_1}{2})|) -g\\frac{3\\Delta t}{2}\\] \\[k_3=(v_{y,t}+\\frac{\\Delta tk_2}{2}) (1-k|\\vec{v}(\\frac{\\Delta tk_2}{2})|) -g\\frac{3\\Delta t}{2}\\] \\[k_4=(v_{y,t}+\\Delta tk_3) (1-k|\\vec{v}(\\Delta tk_3)|) -g2\\Delta t\\]\n\n\nRK4 implementation\nThe most important part of the RK4 implementation has to be the get_slope function refactor\nvoid get_slope(const Vector* slope,Vector* state,const Vector* force, const float drag_per_mass) {\n\n    float vx = state-&gt;elements[2];\n    float vy = state-&gt;elements[3];\n\n    float speed = sqrtf(vx*vx + vy*vy);\n\n    slope-&gt;elements[0] = vx;\n    slope-&gt;elements[1] = vy;\n    slope-&gt;elements[2] = vx*(-speed*drag_per_mass) + force-&gt;elements[2];\n    slope-&gt;elements[3] = vy*(-speed*drag_per_mass) + force-&gt;elements[3];\n}\nand the accompanying changes in the way we step the simulation\nvoid sim_step(Vector* state,const Vector* force, const float dt, const float drag_per_mass) {\n\n    Vector slope = {\n        .elements = (float[4]){},\n        .length = 4\n    };\n\n    get_slope(&slope, state, force, drag_per_mass);\n    vector_scalar_multiply(dt, &slope);\n    vector_add(state, state, &slope);\n}\nwith this, it’s easy to add RK4 by just chaining the calls to get_slope in order to use previous RK estimates for the next ones.\n    get_slope(&k1, state, force, drag_per_mass);\n\n    vector_scalar_multiply_copy(&scratch,0.5*dt, &k1);\n    vector_add(&scratch, state, &scratch); // State + 0.5*dt*k1;\n    get_slope(&k2, &scratch, force, drag_per_mass);\n\n    vector_scalar_multiply_copy(&scratch,0.5*dt, &k2);\n    vector_add(&scratch, state, &scratch); // State + 0.5*dt*k2;\n    get_slope(&k3, &scratch, force, drag_per_mass);\n\n    vector_scalar_multiply_copy(&scratch,dt, &k3);\n    vector_add(&scratch, state, &scratch); // State + dt*k3;\n    get_slope(&k4, &scratch, force, drag_per_mass);\n\n    vector_scalar_multiply(2,&k2);\n    vector_scalar_multiply(2,&k3);\n    vector_add(&slope, &k1, &k2);\n    vector_add(&slope, &slope, &k3);\n    vector_add(&slope, &slope, &k4);\n\n    vector_scalar_multiply(dt/6, &slope);\n    vector_add(state, state, &slope);\nHere’s the comparison of RK4 with a refactored and better version of the forwardEuler.\n\n\n\n\n\n\nNote\n\n\n\nMore specifically, in the new forwardEuler the drag constant doesn’t include dt in it by accident, and it’s easier to track its effect.\nThis had to be done so that we can see how the sims behave with the same drag coefficient.\n\n\n\nWe were promised that RK4 follows the real thing better, and it does here.\nWe know from the initial sims that Forward Euler tends to overshoot when all you do is reduce the velocity (which is what drag and gravity does). Here, we see that RK4 undershoots the Euler estimate- which means it doesn’t have the same overshoot.\n\n\n\nTrajectory without drag\nForward Euler with Drag"
  },
  {
    "objectID": "notes/The Linux Page Fault Handler.html",
    "href": "notes/The Linux Page Fault Handler.html",
    "title": "The Linux Page Fault Handler",
    "section": "",
    "text": "How to get there\nA TLB miss generates an exception. The Linux kernel hooks this into the exception vectors on boot (as functions.\n\n\n\n\n\n\nHooking Exception Vectors in ARM\n\n\n\nThere’s a file named linux/arch/arm/kernel/entry-armv.S which contains unconditional branches to different vector handlers These ASM instructions sit “harmlessly” in the kernel somewhere like 0xC0008000 on boot. They’re copied into the exception vector table using memcpy calls in arch/arm/kernel/traps.c::early_trap_init\n\n\n\n\n\n\n\nmindmap\n    exceptions_init\n        hook_fault_code(do_translation_fault)\n\n\n\n\n\n\n\n\n\nThe Fault Handler in RISCV\n\n\n\n\n\nmindmap\n    handle_page_fault\n        kprobe_page_fault\n        [\"user_mode(regs)\"]\n        trace_page_fault_user\n        \n\n\n\n\n\n\n\n\n\n\n\n\nGlossary\n\n\n\n\nhandle_page_fault - called to handle page faults\nkprobe_page_fault - returns yes if fault has been handled by kprobe\nuser_mode(regs) - macro that refers to the CPU register context saved on stack by the exception service routine\ntrace_page_fault_user - records a trace of the page fault\n\n\n\n__kprobes - linux kernel debug tracer “decorator” macro. BUT if you declare a function as a kprobe (one of the tracers) then it won’t trace it because you don’t want to recurse. This is used in this context to make sure it won’t be traced.\nunlikely() - compiler directive for branch prediction (used in booleans inside if() statements)"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#coverage",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#coverage",
    "title": "Frequency Response of CE Amplifier",
    "section": "Coverage",
    "text": "Coverage\n\n\nFrequency Response of CS/CE Amplifiers\nCE Amplifier with an Output Pole\nCE Amplifier with an Input Pole"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-amplifier-with-an-output-pole",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-amplifier-with-an-output-pole",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE Amplifier with an Output Pole",
    "text": "CE Amplifier with an Output Pole\n\n\nWe begin by analyzing the CE amplifier frequency response.\nAssumptions:\n\n\nIgnore BJT parasitic capacitances (for now).\nAssume \\(r_o \\rightarrow \\infty\\).\nAn external load capacitance \\(C_L\\) dominates the output."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-amplifier-with-an-output-pole-1",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-amplifier-with-an-output-pole-1",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE Amplifier with an Output Pole",
    "text": "CE Amplifier with an Output Pole\n\nKCL @ \\(v_o\\):\n\n\n\\[\\frac{v_o}{R_C || \\frac{1}{sC_L}} + g_m v_i = 0\\]\n\n\nThen, rearranging, \\[A_v(s) = -g_mR_C \\left( \\frac{1}{1 + sR_C C_L} \\right)\\]\n\n\nand we have the form\n\\[A_v(s) = A_0 \\left( \\frac{1}{1 + s/p_1} \\right) \\]"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-poles-a_v",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-poles-a_v",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE with Output Poles: \\(A_V\\)",
    "text": "CE with Output Poles: \\(A_V\\)\n\\[A_v(s) = -g_mR_C \\left( \\frac{1}{1 + sR_C C_L} \\right)\\]"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-pole-r_o",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-pole-r_o",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE with Output Pole: \\(R_o\\)",
    "text": "CE with Output Pole: \\(R_o\\)"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-pole-g_m",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-pole-g_m",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE with Output Pole: \\(G_m\\)",
    "text": "CE with Output Pole: \\(G_m\\)"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-pole-z_i",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-with-output-pole-z_i",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE with Output Pole: \\(Z_i\\)",
    "text": "CE with Output Pole: \\(Z_i\\)"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-transconductance-g_m",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-transconductance-g_m",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE Transconductance \\(G_m\\)",
    "text": "CE Transconductance \\(G_m\\)\n\nRecalling that \\(A_v = -G_m Z_o\\), we can solve for the effective transconductance:\n\n\n\\[G_m = \\frac{-A_v}{Z_o}\\]\n\n\n\\[G_m = \\frac{-\\left[ \\frac{-g_m R_C}{1 + j\\omega R_C C_L} \\right]}{\\frac{R_C}{1 + j\\omega R_C C_L}}\\]\n\n\n\\[G_m = g_m\\]\n\n\nIn this simplified model, the transconductance is frequency independent."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-input-impedance-z_i",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-input-impedance-z_i",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE Input Impedance \\(Z_i\\)",
    "text": "CE Input Impedance \\(Z_i\\)\n\n\n\\[Z_i = \\frac{v_i}{i_i} \\bigg|_{v_o=0}\\]\nBy simple inspection of the small signal model: \\[Z_i = r_\\pi\\]\nThe input impedance is purely resistive and constant across frequency (in this specific configuration)."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-two-port-equivalent-output-pole-only",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#ce-two-port-equivalent-output-pole-only",
    "title": "Frequency Response of CE Amplifier",
    "section": "CE Two-Port Equivalent (Output Pole Only)",
    "text": "CE Two-Port Equivalent (Output Pole Only)\nWe can construct a two-port network representing the amplifier so far:\n\n\n\n\\[Z_i = r_\\pi\\]\n\n\\[G_m = g_m\\]\n\n\\[Z_o = \\frac{R_C}{1 + sR_C C_L}\\]"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#the-ce-amplifier-with-an-input-pole",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#the-ce-amplifier-with-an-input-pole",
    "title": "Frequency Response of CE Amplifier",
    "section": "The CE Amplifier with an Input Pole",
    "text": "The CE Amplifier with an Input Pole\nNow we add a source resistance \\(R_S\\) and a base-emitter capacitance \\(C_\\pi\\).\n\nWe must determine how \\(v_{be}\\) relates to \\(v_i\\) considering the frequency dependent voltage divider at the input."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#deriving-the-input-transfer-function",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#deriving-the-input-transfer-function",
    "title": "Frequency Response of CE Amplifier",
    "section": "Deriving the Input Transfer Function",
    "text": "Deriving the Input Transfer Function\n\nApplying voltage division at the input loop:\n\n\n\\[\\frac{v_{be}}{v_i} = \\frac{Z_\\pi}{Z_\\pi + R_S}\\] where \\(Z_\\pi = r_\\pi \\parallel \\frac{1}{sC_\\pi}\\).\n\n\n\\[\\frac{v_{be}}{v_i} = \\frac{\\frac{r_\\pi}{1 + s r_\\pi C_\\pi}}{\\frac{r_\\pi}{1 + s r_\\pi C_\\pi} + R_S}\\]\n\n\nMultiplying numerator and denominator by \\((1 + s r_\\pi C_\\pi)\\): \\[\\frac{v_{be}}{v_i} = \\frac{r_\\pi}{r_\\pi + R_S + s r_\\pi R_S C_\\pi}\\]"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#deriving-the-input-transfer-function-1",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#deriving-the-input-transfer-function-1",
    "title": "Frequency Response of CE Amplifier",
    "section": "Deriving the Input Transfer Function",
    "text": "Deriving the Input Transfer Function\n\n\\[\\frac{v_{be}}{v_i} = \\frac{r_\\pi}{r_\\pi + R_S + s r_\\pi R_S C_\\pi}\\]\n\n\nWe factor out \\((r_\\pi + R_S)\\) from the denominator to normalize the DC term:\n\n\n\\[\\frac{v_{be}}{v_i} = \\frac{r_\\pi}{r_\\pi + R_S} \\left( \\frac{1}{1 + s C_\\pi \\frac{r_\\pi R_S}{r_\\pi + R_S}} \\right)\\]\n\n\nThis results in an input attenuation factor \\(A_{i}\\) and an input pole \\(\\omega_{p1}\\): \\[\\frac{v_{be}}{v_i} = A_i \\frac{1}{1 + j\\frac{\\omega}{\\omega_{p1}}}\\] where \\(A_i = \\frac{r_\\pi}{r_\\pi + R_S}\\) and \\(\\omega_{p1} = \\frac{1}{C_\\pi (r_\\pi \\parallel R_S)}\\)."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#total-transfer-function",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#total-transfer-function",
    "title": "Frequency Response of CE Amplifier",
    "section": "Total Transfer Function",
    "text": "Total Transfer Function\nWe combine the input section response (\\(A_1\\)) with the output section response (\\(A_2\\)).\n\\[A_v(s) = \\underbrace{\\left( \\frac{v_{be}}{v_i} \\right)}_{\\text{Input Section}} \\cdot \\underbrace{\\left( \\frac{v_o}{v_{be}} \\right)}_{\\text{Output Section}}\\]\n\n\\[A_v(s) = \\left[ \\frac{r_\\pi}{r_\\pi + R_S} \\cdot \\frac{1}{1 + j\\frac{\\omega}{\\omega_{p1}}} \\right] \\cdot \\left[ -g_m R_C \\cdot \\frac{1}{1 + j\\frac{\\omega}{\\omega_{p2}}} \\right]\\]\n\n\n\n\n\n\n\n\nTotal Gain\n\n\n\\[A_v(s) = A_{mid} \\frac{1}{(1 + j\\frac{\\omega}{\\omega_{p1}})(1 + j\\frac{\\omega}{\\omega_{p2}})}\\] \\[\\omega_{p1} = \\frac{1}{C_\\pi (r_\\pi \\parallel R_S)}, \\quad \\omega_{p2} = \\frac{1}{R_C C_L}\\]"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#magnitude-response-two-poles",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#magnitude-response-two-poles",
    "title": "Frequency Response of CE Amplifier",
    "section": "Magnitude Response (Two Poles)",
    "text": "Magnitude Response (Two Poles)\nTypically, the output pole \\(\\omega_{p2}\\) is lower frequency than the input pole \\(\\omega_{p1}\\).\n\n\n\nFlat gain up to \\(\\omega_{p2}\\).\nSlope of \\(-20\\) dB/dec between \\(\\omega_{p2}\\) and \\(\\omega_{p1}\\).\nSlope of \\(-40\\) dB/dec after \\(\\omega_{p1}\\)."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#phase-response",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#phase-response",
    "title": "Frequency Response of CE Amplifier",
    "section": "Phase Response",
    "text": "Phase Response\nThe phase response depends on the separation of the poles.\n\\[\\angle A_v(\\omega) = 180^\\circ - \\tan^{-1}\\left(\\frac{\\omega}{\\omega_{p1}}\\right) - \\tan^{-1}\\left(\\frac{\\omega}{\\omega_{p2}}\\right)\\]\n\n\nWidely Separated If poles are far apart, the phase drops in distinct steps. \n\nClose Proximity If poles are close, the phase shifts merge, creating a steeper descent."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#frequency-dependent-parameters",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#frequency-dependent-parameters",
    "title": "Frequency Response of CE Amplifier",
    "section": "Frequency Dependent Parameters",
    "text": "Frequency Dependent Parameters\nWe can generalize the two-port parameters to include frequency effects:\n\n\nInput Impedance \\(Z_i(\\omega)\\) \\[Z_i(\\omega) = R_S + \\frac{r_\\pi}{1 + j\\omega r_\\pi C_\\pi}\\] Derivation involves simple series/parallel combination of \\(R_S, r_\\pi, C_\\pi\\).\n\nTransconductance \\(G_m(\\omega)\\) \\[G_m(\\omega) = \\frac{g_m}{1 + j\\omega (R_S \\parallel r_\\pi) C_\\pi} \\cdot \\frac{r_\\pi}{r_\\pi + R_S}\\] Effective transconductance rolls off due to the input pole."
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#next-lecture",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#next-lecture",
    "title": "Frequency Response of CE Amplifier",
    "section": "Next Lecture",
    "text": "Next Lecture\n\n\nMiller Capacitance\n\nWhat happens when a capacitor connects Input and Output?\n\nBypass and Coupling Capacitors\n\nEffect on Low Frequency Response"
  },
  {
    "objectID": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#end-of-part-1",
    "href": "slides/ceCsFrequencyResponse/ceCsFrequencyResponse.html#end-of-part-1",
    "title": "Frequency Response of CE Amplifier",
    "section": "End of Part 1",
    "text": "End of Part 1\n\n\n\nFrequency Response of CE Amplifier"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#steps-overview",
    "href": "slides/settingOsPdk/settingOsPdk.html#steps-overview",
    "title": "Setting up an OS PDK",
    "section": "Steps Overview",
    "text": "Steps Overview\nThe following tools require setup for a new PDK:\n\nYosys Synthesis\nOpenROAD (place_opt, global_route, detailed_route, cts)\nMAGIC setup (sets up DRC)\nNETGEN setup (LVS)\nOpenSTA settings\nOpenRCX settings\nXSchem (Symbol creation and spice attachment)\n\nPDK_HOME/libs.tech/xschem"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#resources",
    "href": "slides/settingOsPdk/settingOsPdk.html#resources",
    "title": "Setting up an OS PDK",
    "section": "Resources",
    "text": "Resources\nOpenLane Discussions Issue on the original repo discussing PDK setup: https://github.com/The-OpenROAD-Project/OpenLane/discussions/2133\nPDK Configuration Official documentation on setting up a configuration: https://openlane.readthedocs.io/en/latest/reference/pdk_configuration.html"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#most-important-requirement-config-files",
    "href": "slides/settingOsPdk/settingOsPdk.html#most-important-requirement-config-files",
    "title": "Setting up an OS PDK",
    "section": "Most important requirement: Config files",
    "text": "Most important requirement: Config files\nIn fact, little else matters because these config files point to everything that’s needed. If they’re not there, you have to make them.\n\nRequired Files:\n\nPDK Config: {pdk_root}/{pdk}/libs.tech/librelane/config.tcl\n\nSCL Config: {pdk_root}/{pdk}/libs.tech/librelane/{scl}/config.tcl"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#whats-a-tlef",
    "href": "slides/settingOsPdk/settingOsPdk.html#whats-a-tlef",
    "title": "Setting up an OS PDK",
    "section": "What’s a TLEF?",
    "text": "What’s a TLEF?\nYou don’t actually need it! The configuration file is where you declare the tech lef.\n# Technology LEF\nset ::env(TECH_LEF) \"$::env(PDK_ROOT)/$::env(PDK)/libs.ref/$::env(STD_CELL_LIBRARY)/techlef/$::env(STD_CELL_LIBRARY)__nom.tlef\"\nset ::env(TECH_LEF_MIN) \"$::env(PDK_ROOT)/$::env(PDK)/libs.ref/$::env(STD_CELL_LIBRARY)/techlef/$::env(STD_CELL_LIBRARY)__min.tlef\"\nset ::env(TECH_LEF_MAX) \"$::env(PDK_ROOT)/$::env(PDK)/libs.ref/$::env(STD_CELL_LIBRARY)/techlef/$::env(STD_CELL_LIBRARY)__max.tlef\"\nset ::env(CELLS_LEF) [glob \"$::env(PDK_ROOT)/$::env(PDK)/libs.ref/$::env(STD_CELL_LIBRARY)/lef/*.lef\"]\nset ::env(GDS_FILES) [glob \"$::env(PDK_ROOT)/$::env(PDK)/libs.ref/$::env(STD_CELL_LIBRARY)/gds/*.gds\"]\nset ::env(STD_CELL_LIBRARY_CDL) \"$::env(PDK_ROOT)/$::env(PDK)/libs.ref/$::env(STD_CELL_LIBRARY)/cdl/$::env(STD_CELL_LIBRARY).cdl\"\n\nNote: Except for libs.ref (which is standard), everything else is handled by this config file."
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#configuration-files-detail",
    "href": "slides/settingOsPdk/settingOsPdk.html#configuration-files-detail",
    "title": "Setting up an OS PDK",
    "section": "Configuration Files Detail",
    "text": "Configuration Files Detail\nSource: OpenLane Docs\n\nconfig.tcl: Common info for all SCLs under this PDK. Example\ncommon_pdn.tcl: PDN configuration.\n&lt;scl&gt;/config.tcl: SCL specific info (overrides PDK config). Example\n&lt;scl&gt;/tracks.info: Metal layer offsets and pitches (extracted from tech lef). Example\n\nFormat: &lt;layer name&gt; X|Y &lt;offset&gt; &lt;pitch&gt;\n\n&lt;scl&gt;/synth_exclude.cells: Newline-separated cell names to trim during synthesis. Example\nrcx_patterns.rules: Rules for OpenSTA. Example"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#folder-structure",
    "href": "slides/settingOsPdk/settingOsPdk.html#folder-structure",
    "title": "Setting up an OS PDK",
    "section": "Folder Structure",
    "text": "Folder Structure\n\n\n&lt;pdk_name&gt;\n\nlibs.tech\n\nopenlane\n\nconfig.tcl\ncommon_pdn.tcl (Default PDN script)\n&lt;standard cell library&gt;\n\nconfig.tcl\ntracks.info\nno_synth.cells\ndrc_exclude.cells\n\n\n\nlibs.ref\n\nskyxxx_fd_xx_xx\n\nlef (Can contain techlef declarations)\ntechlef (Doesn’t have to be a folder)\nlib\n…"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#xschem-as-the-virtuoso-equivalent",
    "href": "slides/settingOsPdk/settingOsPdk.html#xschem-as-the-virtuoso-equivalent",
    "title": "Setting up an OS PDK",
    "section": "XSchem as the virtuoso equivalent",
    "text": "XSchem as the virtuoso equivalent\n\nNote: The most basic way to start is skipping XSchem and using ngspice directly.\n\nChecklist:\n\nWrite libs.tech/xschem/xschemrc\nCreate symbols for relevant elements\n\nSuggest starting with nfet and pfet to try a flow.\nInvestigate sky130 symbols (open with XSchem) for reference."
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#xschem-symbols-like-the-cadence-symbol-view",
    "href": "slides/settingOsPdk/settingOsPdk.html#xschem-symbols-like-the-cadence-symbol-view",
    "title": "Setting up an OS PDK",
    "section": "XSchem Symbols (like the Cadence symbol view)",
    "text": "XSchem Symbols (like the Cadence symbol view)\n\n\nYou must create your own symbols (using XSchem).\nYou need to set global attributes for XSchem symbols for them to be parsed into a spice netlist properly.\nGlobal attribute example () is shown on the right (model name: nfet_1v8)\n\n 1 G {}\n 2 K {type=nmos\n 3 lvs_format=\"@spiceprefix@name @pinlist sky130_fd_pr__@model L=@L W=@W nf=@nf m=@mult\"\n 4 format=\"@spiceprefix@name @pinlist sky130_fd_pr__@model L=@L W=@W\n 5 + nf=@nf ad=@ad as=@as pd=@pd ps=@ps\n 6 + nrd=@nrd nrs=@nrs sa=@sa sb=@sb sd=@sd\n 7 + mult=@mult m=@mult\"\n 8 template=\"name=M1\n 9 W=1\n10 L=0.15\n11 nf=1 \n12 mult=1\n13 ad=\\\\\"expr('int((@nf + 1)/2) * @W / @nf * 0.29')\\\\\"\n14 pd=\\\\\"expr('2*int((@nf + 1)/2) * (@W / @nf + 0.29)')\\\\\"\n15 as=\\\\\"expr('int((@nf + 2)/2) * @W / @nf * 0.29')\\\\\"\n16 ps=\\\\\"expr('2*int((@nf + 2)/2) * (@W / @nf + 0.29)')\\\\\"\n17 nrd=\\\\\"expr('0.29 / @W ')\\\\\" nrs=\\\\\"expr('0.29 / @W ')\\\\\"\n18 sa=0 sb=0 sd=0\n19 model=nfet_01v8\n20 spiceprefix=X\n21 \"\n22 drc=\"fet_drc \\{@name\\\\\\\\\\} \\{@symname\\\\\\\\\\} \\{@model\\\\\\\\\\} \\{@W\\\\\\\\\\} \\{@L\\\\\\\\\\} \\{@nf\\\\\\\\\\}\"}"
  },
  {
    "objectID": "slides/settingOsPdk/settingOsPdk.html#magic-setup-checklist",
    "href": "slides/settingOsPdk/settingOsPdk.html#magic-setup-checklist",
    "title": "Setting up an OS PDK",
    "section": "MAGIC setup checklist",
    "text": "MAGIC setup checklist\nBasically, all layout and signoff-related things (except maybe logic equivalence) are done by magic TCL scripts. Checklist:\n\nProcess the given stdcell lefs to create maglefs\nWrite sky130A.tcl\nWrite sky130A.tech\nWrite sky130A.magicrc\nWrite all the custom TCL, like fillgen.\n\n\n\n\n\n\n\nNote\n\n\nI strongly recommend just copying then changing the fill and DRC scripts from the existing sky130/libs.tech/magic or ihp-sg13g2/libs.tech/magic\n\n\n\n\n\n\nSetting up an OS PDK"
  }
]