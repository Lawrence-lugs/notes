<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-11">

<title>Lawrence Roman A. Quizon - Methods in Quantization for TinyML Frameworks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Lawrence Roman A. Quizon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Methods in Quantization for TinyML Frameworks</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul class="task-list">
<li><label><input type="checkbox">TODO: Add commentary, since we’re publicizing this one now. <img src="attachments/wipguy_small.png" class="img-fluid"></label></li>
</ul>
<section id="quantized-averagepool" class="level1">
<h1>Quantized Averagepool</h1>
<p><span class="math inline">\(\sum_i^n r_i=\sum_i^n s(q_i-z)\)</span></p>
<p><span class="math inline">\(=\frac{s[(\sum_i^nq_i)-nz]}{n}\)</span></p>
<p><span class="math inline">\(=\frac{s}{n}\sum q_i-sz\)</span></p>
<section id="accounting-for-the-output-scale" class="level3">
<h3 class="anchored" data-anchor-id="accounting-for-the-output-scale">Accounting for the output scale…</h3>
<p><span class="math inline">\(r_y=\sum_i^n r_{xi}=\sum_i^n s_x(q_{xi}-z_x)\)</span></p>
<p><span class="math inline">\(r_y=\frac{s_x[(\sum_i^nq_{xi})-nz_x]}{n}\)</span></p>
<p><span class="math inline">\(r_y=\frac{s_x}{n}\sum q_{xi}-sz_x\)</span></p>
<p><span class="math inline">\(s_y(q_y-z_y)=\frac{s_x}{n}\sum q_{xi}-sz_x\)</span></p>
<p><span class="math inline">\(q_y=\frac{s_x}{s_yn}\sum q_{xi}-\frac{s_x}{s_y}z_x+z_y\)</span></p>
</section>
</section>
<section id="tflite-quantized-matmul" class="level1">
<h1>TFlite Quantized Matmul</h1>
<p><a href="https://stackoverflow.com/questions/62512871/tensorflow-lite-inference-how-do-i-scale-down-the-convolution-layer-outputs">Tensorflow Lite inference - how do I scale down the convolution layer outputs?</a></p>
<ul>
<li>the output of a multiplication int8 x int8 is also scaled by some multiplier M.
<ul>
<li><span class="math inline">\(Y=AX\)</span></li>
<li><span class="math inline">\(M=\frac{S_AS_X}{S_Y}\)</span> (see google paper on tflite <a href="https://arxiv.org/pdf/1712.05877">https://arxiv.org/pdf/1712.05877</a>)</li>
<li><span class="math inline">\(M\in[0,1]\)</span> empirically.</li>
</ul></li>
<li>we represent this with a bitshift and a number <span class="math inline">\(M_0\in[0.5,1]\)</span>
<ul>
<li><span class="math inline">\(M=2^{-n}M_0\)</span></li>
</ul></li>
<li><span class="math inline">\(M_0\)</span> is representable in fixed-point with <strong>purely fractional bits.</strong></li>
<li>If the initial multiplication result is a 16-bit <span class="math inline">\(R\)</span> and <span class="math inline">\(M_0\)</span> is also 16-bit, then <span class="math inline">\(RM_0\)</span> is a 32-bit number.</li>
<li>We then &gt;&gt; shift out 16 bits (as many as the bits of <span class="math inline">\(M_0\)</span>) of <span class="math inline">\(RM_0\)</span> (as many as the result, then apply <span class="math inline">\(2^{-n}\)</span> to it in shifts.</li>
<li>Finally, apply a saturating clip to 8 bits to the result. (Take the lowest outBits bits)</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>def convert_scale_to_shift_and_m0<span class="op">(</span>scale<span class="op">,</span>precision<span class="op">=</span><span class="dv">16</span><span class="op">):</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">" Convert scale(s) to shift and zero point "</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    shift <span class="op">=</span> <span class="dt">int</span><span class="op">(</span>np<span class="op">.</span>ceil<span class="op">(</span>np<span class="op">.</span>log2<span class="op">(</span>scale<span class="op">)))</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="pp"># </span><span class="er">shift = np.abs(shift)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    m0 <span class="op">=</span> scale <span class="op">/</span> <span class="dv">2</span><span class="op">**</span>shift</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    fp_string <span class="op">=</span> convert_to_fixed_point<span class="op">(</span>m0<span class="op">,</span>precision<span class="op">)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    m0_clipped <span class="op">=</span> fixed_point_to_float<span class="op">(</span>fp_string<span class="op">,</span>precision<span class="op">)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> m0_clipped<span class="op">,</span> shift</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>vconvert_scale_to_shift_and_m0 <span class="op">=</span> np<span class="op">.</span>vectorize<span class="op">(</span>convert_scale_to_shift_and_m0<span class="op">)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>def convert_to_fixed_point<span class="op">(</span>number<span class="op">,</span>precision<span class="op">):</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">" Convert a float [0,1] to fixed point binary string"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="ch">'</span><span class="er">'</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i in range<span class="op">(</span>precision<span class="op">):</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        number <span class="op">*=</span> <span class="dv">2</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        integer <span class="op">=</span> <span class="dt">int</span><span class="op">(</span>number<span class="op">)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        number <span class="op">-=</span> integer</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> str<span class="op">(</span>integer<span class="op">)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>def convert_to_fixed_point_int<span class="op">(</span>number<span class="op">,</span>precision<span class="op">):</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">" Convert a float [0,1] to fixed point binary "</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dt">int</span><span class="op">(</span>convert_to_fixed_point<span class="op">(</span>number<span class="op">,</span>precision<span class="op">),</span>base<span class="op">=</span><span class="dv">2</span><span class="op">)</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>def fixed_point_to_float<span class="op">(</span>number<span class="op">,</span>precision<span class="op">):</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">" Convert a fixed point binary to float [0,1] "</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i in range<span class="op">(</span>precision<span class="op">):</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> <span class="dt">int</span><span class="op">(</span>number<span class="op">[</span>i<span class="op">])</span> <span class="op">*</span> <span class="dv">2</span><span class="op">**-(</span>i<span class="op">+</span><span class="dv">1</span><span class="op">)</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>m0<span class="op">,</span> shift <span class="op">=</span> q<span class="op">.</span>convert_scale_to_shift_and_m0<span class="op">(</span><span class="fl">0.019110053777694702</span><span class="op">)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>fp_int <span class="op">=</span> q<span class="op">.</span>convert_to_fixed_point_int<span class="op">(</span>m0<span class="op">,</span><span class="dv">16</span><span class="op">)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="op">(</span>m0 <span class="op">*</span> <span class="dv">2</span><span class="op">**(</span>shift<span class="op">))</span> <span class="op">-</span> <span class="op">(</span>fp_int <span class="op">*</span> <span class="dv">2</span><span class="op">**(</span>shift<span class="op">-</span><span class="dv">16</span><span class="op">))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="zeroes-thereof" class="level3">
<h3 class="anchored" data-anchor-id="zeroes-thereof">Zeroes Thereof</h3>
<p><a href="attachments/16d40c87b91d970d6014377f61dc9f01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="attachments/16d40c87b91d970d6014377f61dc9f01.png" class="img-fluid"></a></p>
</section>
</section>
<section id="full-integer-quantized-addition" class="level1">
<h1>Full Integer Quantized Addition</h1>
<p><a href="https://openaccess.thecvf.com/content_cvpr_2018/Supplemental/0777-supp.pdf">openaccess.thecvf.com</a> — Supplementary information of the TFLite paper</p>
<blockquote class="blockquote">
<p>Some neural networks use a plain <strong>Addition</strong> layer type, that simply adds two activation arrays together. Such Addition layers are more expensive in quantized inference compared to floating-point because rescaling is needed: one input needs to be rescaled onto the other’s scale using a fixedpoint multiplication by the multiplier <span class="math inline">\(M = S_1/S_2\)</span> similar to what we have seen earlier before actual addition can be performed as a simple integer addition; finally, the result must be rescaled again to fit the output array’s scale8.</p>
</blockquote>
<p><em>Their implementation in the tflite code: </em></p>
<p><a href="https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h#L1402-L1507">https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h#L1402-L1507</a></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(;</span> i <span class="op">&lt;</span> size<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 input1_val <span class="op">=</span> input1_offset <span class="op">+</span> input1_data<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 input2_val <span class="op">=</span> input2_offset <span class="op">+</span> input2_data<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 shifted_input1_val <span class="op">=</span> input1_val <span class="op">*</span> <span class="op">(</span><span class="dv">1</span> <span class="op">&lt;&lt;</span> left_shift<span class="op">);</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 shifted_input2_val <span class="op">=</span> input2_val <span class="op">*</span> <span class="op">(</span><span class="dv">1</span> <span class="op">&lt;&lt;</span> left_shift<span class="op">);</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 scaled_input1_val <span class="op">=</span> MultiplyByQuantizedMultiplierSmallerThanOne<span class="op">(</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        shifted_input1_val<span class="op">,</span> input1_multiplier<span class="op">,</span> input1_shift<span class="op">);</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 scaled_input2_val <span class="op">=</span> MultiplyByQuantizedMultiplierSmallerThanOne<span class="op">(</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        shifted_input2_val<span class="op">,</span> input2_multiplier<span class="op">,</span> input2_shift<span class="op">);</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 raw_sum <span class="op">=</span> scaled_input1_val <span class="op">+</span> scaled_input2_val<span class="op">;</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 raw_output <span class="op">=</span> MultiplyByQuantizedMultiplierSmallerThanOne<span class="op">(</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>                                 raw_sum<span class="op">,</span> output_multiplier<span class="op">,</span> output_shift<span class="op">)</span> <span class="op">+</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                             output_offset<span class="op">;</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> int32 clamped_output <span class="op">=</span> <span class="bu">std::</span>min<span class="op">(</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        output_activation_max<span class="op">,</span> <span class="bu">std::</span>max<span class="op">(</span>output_activation_min<span class="op">,</span> raw_output<span class="op">));</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    output_data<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="kw">static_cast</span><span class="op">&lt;</span>uint8<span class="op">&gt;(</span>clamped_output<span class="op">);</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="errors-based-on-the-tflite-output-scaling" class="level1">
<h1>Errors based on the TFLite output scaling</h1>
<ul>
<li>Graph
<ul>
<li>x — internal precision</li>
<li>y — order of error</li>
<li>(16, ^-10)</li>
</ul></li>
</ul>
</section>
<section id="google-paradigm-on-dealing-with-low-precision-matrix-multiplication" class="level1">
<h1>Google Paradigm on Dealing with Low Precision Matrix Multiplication</h1>
<p>Also see:</p>
<p>— Basically also discussed in the TFLite paper</p>
<p><a href="https://github.com/google/gemmlowp/blob/master/doc/quantization.md">https://github.com/google/gemmlowp/blob/master/doc/quantization.md</a></p>
</section>
<section id="litert-torch-converter" class="level1">
<h1>LiteRT Torch Converter</h1>
<ul>
<li>Can convert with quantization using either torch’s PT2E or TFLite.
<ul>
<li>See https://github.com/google-ai-edge/ai-edge-torch/blob/main/docs/pytorch_converter/README.md</li>
</ul></li>
<li></li>
</ul>
</section>
<section id="on-edmips" class="level1">
<h1>On EdMIPS</h1>
<p><a href="https://www.figma.com/file/9pmqozMDAtTJckTQEhCzhj/300B?type=whiteboard&amp;node-id=21-135&amp;t=8svuD9WRbEzFTHpY-4">https://www.figma.com/file/9pmqozMDAtTJckTQEhCzhj/300B?type=whiteboard&amp;node-id=21-135&amp;t=8svuD9WRbEzFTHpY-4</a></p>
</section>
<section id="biases" class="level1">
<h1>Biases</h1>
<p><span class="math inline">\(\hat{I}\cdot\hat{W}+\hat{B}\neq I\cdot W+B\)</span></p>
</section>
<section id="static-vs-dynamic-quantization" class="level1">
<h1>Static vs Dynamic Quantization</h1>
<p>The value distribution of the input feature maps can vary from input to input, making it difficult to specify optimal quantization ranges.</p>
<section id="dynamic-quantization-specify-quantization-range-for-each-activation-fmap-before-calculation." class="level3">
<h3 class="anchored" data-anchor-id="dynamic-quantization-specify-quantization-range-for-each-activation-fmap-before-calculation.">Dynamic Quantization — Specify quantization range for each activation fmap before calculation.</h3>
</section>
<section id="static-quantization-specify-range-beforehand" class="level3">
<h3 class="anchored" data-anchor-id="static-quantization-specify-range-beforehand">Static Quantization — Specify range beforehand</h3>
<ul>
<li>Use a series of calibration inputs to compute the typical range using:
<ul>
<li>Minimize MSE</li>
<li>KL Divergence (entropy)</li>
<li>Impose clipping range during training</li>
</ul></li>
</ul>
</section>
<section id="works" class="level3">
<h3 class="anchored" data-anchor-id="works">Works</h3>
<ul>
<li>LQNets</li>
<li>PACT</li>
<li>LSQ/LSQ+</li>
</ul>
</section>
</section>
<section id="stochastic-quantization" class="level1">
<h1>Stochastic Quantization</h1>
<blockquote class="blockquote">
<p><em>Typically only for online learning</em></p>
</blockquote>
<p>For online learning with quantization, small weight updates may not necessarily lead to large changes in weight. To effectively still reflect the small changes, we can keep a probability <span class="math inline">\(x\)</span> separate from the rounded <span class="math inline">\(\lfloor x \rceil\)</span> integer x. In this case,</p>
<p><span class="math inline">\(\hat{x} = \begin{cases}
\lfloor x \rfloor \text{ with probability } \lceil x \rceil - x  \\
\lceil x \rceil \text{ with probability } x - \lfloor x \rfloor\\
\end{cases}\)</span></p>
</section>
<section id="simulated-integer-only-quantization" class="level1">
<h1>Simulated &amp; Integer-only Quantization</h1>
<blockquote class="blockquote">
<p><em>In simulated quantization, the quantized model parameters are stored in low-precision, but the operations (e.g.&nbsp;matrix multiplications and convolutions) are carried out with floating point arithmetic. - Gholami ’22 LPCV</em></p>
</blockquote>
<p><a href="attachments/9cdf206f1a19dd8d055e338037c60b6f.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="attachments/9cdf206f1a19dd8d055e338037c60b6f.png" class="img-fluid"></a></p>
<p>Simulated Quantization is “fake” quantization needing dequantization before every operation. EdMIPS uses simulated quantization in order to perform their differential architecture search with finetuning while searching for the optimal weight quantization set <span class="math inline">\(\{o_i,o_w\}\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
+ Fact: The HWGQ Quantizer from EdMIPS is simulated quantization. See the <code>*step</code> portion of the formula.
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb4"><pre class="sourceCode javascript code-with-copy"><code class="sourceCode javascript"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">class</span> <span class="fu">_hwgq</span>(torch<span class="op">.</span><span class="at">autograd</span><span class="op">.</span><span class="at">Function</span>)<span class="op">:</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>     @staticmethod</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>     def <span class="fu">forward</span>(ctx<span class="op">,</span> x<span class="op">,</span> step)<span class="op">:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>         y <span class="op">=</span> torch<span class="op">.</span><span class="fu">round</span>(x <span class="op">/</span> step) <span class="op">*</span> step</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>         <span class="cf">return</span> y</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>     @staticmethod</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>     def <span class="fu">backward</span>(ctx<span class="op">,</span> grad_output)<span class="op">:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>         <span class="cf">return</span> grad_output<span class="op">,</span> None</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>In contrast, works like MCUNet and AIMC use integer-only quantization specifically for the reason of energy efficiency.</p>
<blockquote class="blockquote">
<p><em>Some hardware processors, including NVIDIA V100 and Titan RTX, support fast processing of low-precision arithmetic that can boost the inference throughput and latency. — Gholami ’22</em></p>
</blockquote>
<p>Peng ’21 Neurosim in a work on fully-integer based quantization for mobile CNN inference mention in their introduction:</p>
<blockquote class="blockquote">
<p>… the dot product of two bit vectors x and y can be computed using the following formula [24], where <span class="math inline">\(x_i,y_i\in\{0,1\}\forall i\)</span> and the ’’bitcount” operation counts the number of bits that have a value of one in each vector element</p>
</blockquote>
<p><span class="math inline">\(x\cdot y=bitcount(x AND y)\)</span></p>
<p>This is exactly how we perform 1b W x 1b I in SRAM-IMC.</p>
</section>
<section id="determining-steprange-vs-bits" class="level1">
<h1>Determining step/range vs bits</h1>
<!-- Column 1 -->
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_model_weights(model, bits, step):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># qm = EdMIPS quantization module</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    quantized_state_dict <span class="op">=</span> model.state_dict()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        quantized_state_dict[name] <span class="op">=</span> qm._gauss_quantize.<span class="bu">apply</span>(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            param,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            step,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            bits</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(quantized_state_dict)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!-- Column 2 -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="attachments/6fe497340003c358ccc0bd8993638575.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" data-glightbox="description: .lightbox-desc-3" title="The first layer of DS-CNN quantized with a 4-bit Gaussian Quantizer with step=0.336. You can see that 16 levels are available."><img src="attachments/6fe497340003c358ccc0bd8993638575.png" class="img-fluid figure-img" alt="The first layer of DS-CNN quantized with a 4-bit Gaussian Quantizer with step=0.336. You can see that 16 levels are available."></a></p>
<figcaption>The first layer of DS-CNN quantized with a 4-bit Gaussian Quantizer with step=0.336. You can see that 16 levels are available.</figcaption>
</figure>
</div>
</section>
<section id="edmips-quantizer-range" class="level1">
<h1>EdMIPS Quantizer Range</h1>
<p>From these two and the corresponding code, we now have a working quantizer function.</p>
<p>We now need to look into the literature in order to find the optimal quantizer range. IIRC, this is mentioned in:</p>
<ul>
<li>HWGQ (though this is for 1-bit)</li>
<li>EdMIPS <span class="math inline">\((\sigma t_i,\sigma t_{i+1})\)</span></li>
</ul>
<blockquote class="blockquote">
<p><em>A quantizer is the set of ranges for which you clip values into specific integers.</em></p>
</blockquote>
<p>The optimal quantizer can be found by way of Lloyd’s Algorithm (k-means clustering). However, this is difficult to perform for since each weight group has a different distribution. This is simplified by two ideas:</p>
<ol type="1">
<li>Weight distributions are typically gaussian. Hence, we only need the mean and variance in order to specify the quantizer.</li>
<li>By using batchnorm, the weight means and variances are all 0 and 1, allowing us to use the same quantizer for all layers. This means that we only have to solve the Lloyd’s algorithm once. Lloyd’s algorithm restrained for uniform quantization for standard gaussian <span class="math inline">\(\sigma=1,\mu=0\)</span> gives results as follows:</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>gaussian_steps <span class="op">=</span> {<span class="dv">1</span>: <span class="fl">1.596</span>, <span class="dv">2</span>: <span class="fl">0.996</span>, <span class="dv">3</span>: <span class="fl">0.586</span>, <span class="dv">4</span>: <span class="fl">0.336</span>}</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>hwgq_steps <span class="op">=</span> {<span class="dv">1</span>: <span class="fl">0.799</span>, <span class="dv">2</span>: <span class="fl">0.538</span>, <span class="dv">3</span>: <span class="fl">0.3217</span>, <span class="dv">4</span>: <span class="fl">0.185</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li>EdMIPS further assumes that the means are all 0 but the variances are non-1. This means that measuring the standard deviation per parameter set (weight layer) is enough. The step for the quantization can then be scaled as <span class="math inline">\(\sigma*step\)</span>. Hence, the gauss quantize function becomes as follows.</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _gauss_quantize(torch.autograd.Function):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(ctx, x, step, bit):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        lvls <span class="op">=</span> <span class="dv">2</span> <span class="op">**</span> bit <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> x.std().item()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        step <span class="op">*=</span> alpha</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> (torch.<span class="bu">round</span>(x<span class="op">/</span>step<span class="op">+</span><span class="fl">0.5</span>)<span class="op">-</span><span class="fl">0.5</span>) <span class="op">*</span> step</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        thr <span class="op">=</span> (lvls<span class="op">-</span><span class="fl">0.5</span>)<span class="op">*</span>step</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.clamp(<span class="bu">min</span><span class="op">=-</span>thr, <span class="bu">max</span><span class="op">=</span>thr)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(ctx, grad_output):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grad_output, <span class="va">None</span>, <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!-- Column 1 -->
<p>We can observe there’s a constant <code>-step*0.5</code> portion to the y, which shows itself as the lack of a “0” in our quantized weight histograms.</p>
<!-- Column 2 -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../08_Literature_Reference/Papers/Jacob ‘18 CVPR “Quantization &amp; Training of NN for Efficient INT-only Arithmetic” synced block.md" class="lightbox" data-gallery="quarto-lightbox-gallery-4" data-glightbox="description: .lightbox-desc-4" title="Jacob ’18 CVPR “Quantization &amp; Training of NN for Efficient INT-only Arithmetic” synced block"><embed src="../08_Literature_Reference/Papers/Jacob ‘18 CVPR “Quantization &amp; Training of NN for Efficient INT-only Arithmetic” synced block.md" class="img-fluid"></a></p>
<figcaption>Jacob ’18 CVPR “Quantization &amp; Training of NN for Efficient INT-only Arithmetic” synced block</figcaption>
</figure>
</div>


<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-3">The first layer of DS-CNN quantized with a 4-bit Gaussian Quantizer with step=0.336. You can see that 16 levels are available.</span>
<span class="glightbox-desc lightbox-desc-4">Jacob ’18 CVPR “Quantization &amp; Training of NN for Efficient INT-only Arithmetic” synced block</span>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"openEffect":"zoom","loop":false,"closeEffect":"zoom","selector":".lightbox","descPosition":"bottom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




</body></html>