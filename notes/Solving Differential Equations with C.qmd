---
title: Solving Differential Equations with C
date: 2026-01-10
modified: 2026-01-11T11:37:08+08:00
---

::: {.callout-important}

- People have definitely come up with differential equations long before computers were around
- Differential equations are sometimes so hard there aren't any analytical solutions
- Computers were probably a lifesaver when they came around
- How did people start using computers and developing numeric algorithms to solve differential equations?
- I want to try it in C

:::

# First solutions: Runge-Kutta

Actually, instead of just coming up with hard-to-solve differential equations, they also came up with pain-in-the-ass-to-compute algorithms to solve them (Runge-Kutta, Euler) earlier than computers.

::: {.callout-tip title="[Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods)"}
Given a problem in the following form: 
$$\frac{dy}{dt}=f(t,y), y(t_0)=y_0$$
RK4 gives an estimate of $\frac{dy}{dt}$ using a weighted average of four slopes $f(t,y)$ $k_1$ to $k_4$ in the "future" of the current point in time
Choosing a time step $\Delta t>0$
$$y_{n+1}=y_n+\frac{\Delta t}{6}(k_1+2k_2+2k_3+k_4)$$
$$t_{n+1}=t_n+\Delta t$$
$$k_1=f(t_n,y_n),$$
$$k_2=f(t_n+\frac{\Delta t}{2},y_n+\Delta t\frac{k_1}{2})$$
$$k_3=f(t_n+\frac{\Delta t}{2},y_n+\Delta t\frac{k_2}{2})$$
$$k_4=f(t_n+\Delta t,y_n+\Delta tk_3)$$
:::

### Dragged Ballistics

RK4 was first useful in calculating ballistics for weapons. Real trajectories had to be calculated with drag and wind, which wasn't easy to hand-calculate. With drag:

$$\frac{d^2x}{dt^2}=-\frac{k}{m}\sqrt{v_x^2+v_y^2}v_x$$
$$\frac{d^2x}{dt^2}=-g-\frac{k}{m}\sqrt{v_x^2+v_y^2}v_y$$
But wait... this is second order.

### The Cheat: Everything is First-Order In Phase Space

We can cheat by analytically reducing this into a first-order differential equation solvable by RK4.

So then, we merge the entire state space into one four-dimensional variable:
$$S = \begin{bmatrix}
x \\ y \\ v_x \\ v_x
\end{bmatrix}$$

::: {.callout-tip title="Trivia"}
In more usual terms, this is called changing into the *phase space*. Apparently *phase* is used this way as in *status* (phases of the moon). To me, that naming feels discrete, though.
:::

This turns the second-order 2D problem into a first-order 4D problem
$$\frac{dS}{dt}=\begin{bmatrix}
dx/dt \\ dy/dt \\ dv_x/dt \\ dv_y/dt 
\end{bmatrix}=\begin{bmatrix}
v_x \\ v_y \\ a_x \\ a_y
\end{bmatrix}$$
$$\frac{dS}{dt}=\begin{bmatrix}
dx/dt \\ dy/dt \\ dv_x/dt \\ dv_y/dt 
\end{bmatrix}=\begin{bmatrix}
v_x \\ v_y \\ -\frac{k}{m}\sqrt{v_x^2+v_y^2}v_x \\ -g-\frac{k}{m}\sqrt{v_x^2+v_y^2}v_y
\end{bmatrix}$$
For which we can apply RK4.

TODO: Implement this solver in C with GNUplot as the plotter

```c
```

### Correction: Every N-order ODE is a higher dimension 1st-order ODE

However, not all problems only change versus one variable. In the prettiest (colorful) problems, we're tracking a scalar or a vector field permeating all across space but also changing across time.

So say, the 1D heat equation:
$$\frac{\partial u}{\partial t} - \frac{\partial^2 u}{\partial x^2} = f$$

This is a partial differential equation (PDE) with both time and space dimension. 

In this case, the way they were able to do it is to separate space into a discrete grid. We want to satisfy the time portion of the PDE at specific points in the grid:
$$\frac{d u_i}{dt} - \frac{u_{i+1} - 2u_i + u_{i-1}}{\Delta x^2} = f_i$$
We choose a $\Delta x$ for the grid, then we choose a $\Delta t$ to apply RD4 to it.

In this grid, the neighboring points influence point $i$. This is kinda reminiscent of trying to simulate phase coupled oscillators like in the [Kuramoto model](https://en.wikipedia.org/wiki/Kuramoto_model).

Problems:

- This blew the problem to as many dimensions as there are $\text{grid cells} \cdot \text{ODE order}$  
- Trying to get the EXACT solution for each point is difficult when each point doesn't even know if it's in the correct position yet. This caused a soft of overfitting in this approach where it's hard to converge in situations where the gradient changes quickly. You can fix this by making $dx$ small enough that the change is still "gradual", but this gets expensive quick.

### Into finite elements

The form of the finite difference equation
$$\frac{d u_i}{dt} - \frac{u_{i+1} - 2u_i + u_{i-1}}{\Delta x^2} = f_i$$
is extremely rigid. We have to enforce $f_i$ (heat source), but then it's almost never correct either.

Instead, we can rearrange to accept a finite error over the entire domain:
$$R(x)=\frac{d u}{dt} - \frac{d^2u}{dx^2}-f_i$$
and "kindly ask" this residual to approach zero, instead of forcing it all to zero every time step. In other words, we're able to relax the condition so that a grid point $i$ doesn't reduce its error to 0 at the cost of other points being unable to converge or getting a big relative error.
$$\int_\Omega{R(x)\cdot v(x)dx=0}$$
::: {.callout-warning}
That explanation feels wishy-washy about "kindly asking" and "giving a weaker condition", sounding like empirical AI. Probably some better way to say it after the next part.
:::


$v(x)$ is some arbitrary "test function", useful to give nice properties to the integral. I would think that $v(x)$ has to be nonzero everywhere (else $R(x)$ can be nonzero for some $x$).

With $v(x)$ we can apply integration by parts:
$$\int_\Omega{(\frac{d u}{dt} - \frac{d^2u}{dx^2}-f_i)\cdot v(x)dx=0}$$
$$\int_\Omega{\frac{d^2u}{dx^2}\cdot v(x)dx=0}$$