---
title: "LLMs can be central nervous systems?"
date: "2025-10-03"
---

Apparently AI is shifting towards trying to use LLMs in different manners like

- RL + LLM
- Vision + LLM (Recall, or something)
- LLMs for controlling robots

# AI is evolving like animals...?

This may be reminiscent of biological evolution of nervous systems. Quick reflex-like signalling systems may have evolved into peripheral + central nervous systems. 

::: {.callout-tip title="Fact-check!"}
Northcutt, R. Glenn. "Evolution of centralized nervous systems: two schools of evolutionary thought." Proceedings of the National Academy of Sciences 109.supplement_1 (2012): 10626-10633.
Some early creatures have *ganglionated cephalic nervous systems*. *Diffuse nerve plexus* are 
:::

Creatures with central nervous systems are allowed to have an internal cognition network with latent representations and statefulness that can help perform long term tasks.

# Replacing AlphaStar's Core Module with an LLM

So then maybe we can make AlphaStar into a true ladder-breaking animal

What would happen if, say, we replaced AlphaStar's Deep LSTM core module with an LLM instead now?

![Vinyals, Oriol, et al. "Grandmaster level in StarCraft II using multi-agent reinforcement learning." _nature_ 575.7782 (2019): 350-354.](attachments/Pasted%20image%2020260109221438.png)

The problem with this is that LLMs are still *stateless*. They don't retain memory like an LSTM does. So we need LLMs with memory.

## Approaches to Stateful LLM 

With some effort to arrange "evolutionarily":

- Neuro-sama (some guy named Vedal) - Probably a text file used with the context window.
- Recurrent Memory Transformers (NeurIPS 2022) - Add special memory tokens to the input sequence and output sequence. Then, eat your own output.
	- Kind of like *Ghajini/Memento* in a way. You don't remember anything, but you do take notes.
- Structured State Space Models (various) - compressed latent space representation memory, but still explicit memory.
	- Mamba - They turned SSM parameters *dependent* on input $P(x)$.
- Titans (Google 2024) - something like a high learning rate module (contextual memory) turning the medium-term context into parameters. Might be the most biological of all of these?

![Titans' Memory as a Context (MAC)](attachments/Pasted%20image%2020260109224301.png)