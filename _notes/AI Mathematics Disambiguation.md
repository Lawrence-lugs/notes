---
title: AI Mathematics Disambiguation
subtite: Those that appear in NeurIPS and such
modified: 2026-01-17
date: 2026-01-17
draft: true
---

> [!tip] How we got there
> 
> - Information geometry described biological learning in a differential geometry type of manner.
> - This later became relevant in the foundations of gradient descent and the mathematical analysis thereof.
> - However, information geometry wasn't good at describing the fractal "attractor" (learning trajectory) and sparse connectivity of biological neural systems.
> - In contrast, the structured connectivity of AI was a great place for information geometry. Hence, nowadays, IG can typically be found in "Advanced Optimization for ML" and other such courses.

>[!note] References
>- Surya Ganguli - *Statistical Mechanics of Learning*, Stanford
>- Pratik Chaudhari - *Principles in Deep Learning*, UPenn
>- Shun-Ichi Amari - *Information Geometry and its Applications*
>- Various (ICL, NYU, Qualcomm, Deepmind) - *[Geometric Deep Learning](https://arxiv.org/pdf/2104.13478)* 

Information Geometry comes from the differential mathematics side of AI.
The other sides are rigorous statistics, dynamics (edge of chaos, etc), and physicists ("phase transition" and such).